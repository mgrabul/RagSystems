{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic\n",
    " <!-- - Define project topic for research - one-sentence description; -->\n",
    "Extending the capabilities of LLMs using Retrieval-Augmented Generation techniques. Comparing the accuracy of answering questions between standard LLM vs. RAG LLM for different LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "This research aims to enhance the capabilities of Large Language Models (LLMs) using Retrieval-Augmented Generation (RAG) techniques. By comparing the accuracy of question-answering between standard LLMs and those integrated with RAG systems, this study explores the potential improvements in performance and robustness that RAG systems can offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "LLMs are quite useful and have taken the world of AI by storm. They have extensive usages, and we are still rediscovering what they can be used for. However, there are some aspects that are not ideal, for example:\n",
    "\n",
    "- **Limitation in Memory**: LLMs are trained once on a set of data. Once trained, they cannot answer questions or generate text on data they have not been trained on (problem of underfitting and high bias).\n",
    "- **Hardware Requirements and Cost**: Running useful general-purpose LLM requires serious hardware capabilities, which comes with additional costs.\n",
    "- **Privacy**: Large and useful LLMs are typically run by companies, requiring users to send data to externally managed servers. This may pose problems for individual users who need to submit private data and for companies, especially with GDPR requirements that might not be satisfied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Retrieval augmented generation\n",
    "\n",
    "\n",
    "*Retrieval-augmented generation (RAG) is an advanced artificial intelligence (AI) technique that combines information retrieval with text generation, allowing AI models to retrieve relevant information from a knowledge source and incorporate it into generated text.*[6]\n",
    "\n",
    "\n",
    "The basic idea around a RAG system is extending the capabilities of an already trained machine learning model (MLM) in particular an LLM. \n",
    "\n",
    "One common problem in llms is that inorder get a correct output/answer the LLM needs to be trained on a dataset that is relevant to the input/question. For example, asking an LLM who is the winner of 2024 euro championship will not result in a correct answer if the model is trained before 2024. Regardless of the capabilities of the chosen LLM, it just does not contain memory related to this information. In case like this the LLM will need to be retrained. Different problem that ocures in llms and machine models in general is that perhaps the LLM is not powerfull enght for the given task. Example for the second case is that if a model is asked extreamly difficult question, even though the model has been trained on all the relevant data it might still not produce correct output in case of a difficult task.\n",
    "\n",
    " Rag systems do not suffer as much as other models from this problem, their memory can be extended without the need for training or adding additional parameters. \n",
    " The system has two types of memory:\n",
    " \n",
    "- **Parametric memory** - refers to the knowledge in the LLM generative model \n",
    "- **Non-parametric memory** - stored in special types of databases. \n",
    "This non-parametric memory can be extended with additional knowledge easily and it still can be used by the LLM. \n",
    "**This is the core idea in RAG. Without any additional training new knowledge can be made available to the LLM.**\n",
    "\n",
    "\n",
    "The RAG technique integrates a retrieval model and generative model. RAG systems usually work around extending the capabilities of NLP(Natural Language processing models) giving them additional memory or information on various topics. Therefor a production RAG system in the context of LLMs is build from 2 sub-systems:\n",
    "\n",
    "\n",
    "* **Retrieval and Augmentation**\n",
    "* **Generative AI component** or parametric memory, it is usually language generation tool like LLMs\n",
    "\n",
    "\n",
    "From the perspective of usage of the RAG system, generating an answer is done in three phases:\n",
    "\n",
    "- Phase 1 Retrieval, Knowledge retrieval, When asked a question the RAG retrieval system will retrieve all known information by submitting an appropriate query to the non-parametric memory.\n",
    "- Phase 2 Augmentation, with the retrieved information, a context around the question is created, the context should contain all information for answering the question.\n",
    "- Phase 3 Generation, The context and question will be passed to an LLM that will generate an answer.\n",
    "\n",
    "```\n",
    "Question  \n",
    "    |\n",
    "     ---> | Retrieval and | \n",
    "          | Augmentation  |\n",
    "                 |\n",
    "                  --> (context + question)\n",
    "                               |\n",
    "                                ---> | Generative |\n",
    "                                     |     AI     |\n",
    "                                     |  Compnent  | --> Answer             \n",
    "              \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "## Generative AI component or a LLM transformer\n",
    "\n",
    "\n",
    "The Generative AI component is an LLM transformer. Typically, the transformer has two main components, the encoder and the decoder. The input text is initially tokenized, meaning it is split into small continuous lists of characters. In the next step, the model transforms the tokens into fixed-size vectors called embeddings. In the final step, in a so-called attention layer, an additional linear transformation on the embeddings is performed, making them dependent on the context they appear in. These context-dependent embeddings, also called contextual embeddings, are the output of the encoder and the input to the decoder. The encoder is of extreme importance in this text as it is a crucial part of a RAG system.\n",
    "\n",
    "\n",
    "*Embeddings are vector representations, typically produced by a neural network, whose main objective is to map (embed) the input media item into a vector space, where the locality encodes the semantics of the input* [2].\n",
    "\n",
    "\n",
    "The decoder takes the input and generates new embeddings and again combines the output with the embedded tokens to produce new tokens. The vector transformation where token embeddings are transformed to different ones to capture the context of the input text is called attention. It is described in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762):\n",
    "As a result of the initial research paper on transformers:\n",
    "The LLM transformers do not suffer from maintaining context and dependencies as much as recurrent or convolutional networks.\n",
    "\n",
    "\n",
    "*Transformers can be trained significantly faster than architectures based on recurrent or convolutional layers.*\n",
    "*Transformers outperform the best previously reported models in translation.* [1]\n",
    "\n",
    "\n",
    "The quality of the generated text generally depends on the number of the parameters of the model.\n",
    "Regardless of the quality of the output, every LLM for a given input is expected to generate grammatically correct text and semantically related to the input. \n",
    "For example if i ask even the smallest LLM \"What is the capital of France?\" it might return \"Berlin is the capital of France\". \n",
    "The answers will perhaps not be correct but they will contain the context of the question and grammatically correct.\n",
    "\n",
    "\n",
    "**The generative component of the RAG system needs to be able for a given input and context to generate grammatically correct sentences related to the given context.**\n",
    "\n",
    "**This is exactly what is achieved with the LLM transformers and that is why they are ideal to be used in the generation phase of the RAG system.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "## Retrieval System\n",
    "\n",
    "The retrieval system is used to store and retrieve knowledge outside of the knowledge maintained in the LLM. The knowledge here is in the form of paragraphs, sentences, or other forms of continuous text, referred to here as a text block. The text blocks are stored as embeddings.\n",
    "\n",
    "This part of the system is composed of two components or modules:\n",
    "\n",
    "* Sentence Embedding Model\n",
    "* Non-parametric memory\n",
    "\n",
    "\n",
    "### Storing Phase:\n",
    "The idea of the retrieval phase is for a given text block to create embedding and store this embedding in the nonparametric memory.\n",
    "\n",
    "The process is as follows: \n",
    "\n",
    "1. Take a text block, sentence, or paragraph.\n",
    "2. Passes this text to the Sentence Embedding model to get an embedding(dense vector).\n",
    "3. Stores this embedding in a non-parametric memory.\n",
    "\n",
    "```\n",
    "Sentence\n",
    "|\n",
    " -> Tokenization\n",
    "     |\n",
    "      -> Embedded Vectors\n",
    "          |\n",
    "           -> Contextual Vectors\n",
    "```\n",
    "\\newpage\n",
    "\n",
    "### Retrieval Phase:\n",
    "The idea of the retrieval phase is to return all relevant information from the non-parametric memory for a given text, query, or question.\n",
    "\n",
    "The process is as follows: \n",
    "\n",
    "1. Takes input text, referred to as a query.\n",
    "2. Passes the query text to the Sentence Embedding model to get a query embedding or a query dense vector.\n",
    "3. Using the query embedding, fetches a list of related knowledge from the non-parametric memory.\n",
    "\n",
    "```\n",
    "Question \n",
    "   |\n",
    "    -> Sentence Embeder \n",
    "   |            | \n",
    "   |             -> Embeding \n",
    "   |                 |\n",
    "   |                  -> Non-Parametric memory \n",
    "   |                         |\n",
    "   |                          -> Retreaved Documents\n",
    "   |                                   |\n",
    "   |                                    -> Context\n",
    "   |                                          | \n",
    "   |                                           -->| Generative |\n",
    "    --------------------------------------------->| Component  |\n",
    "                                                        |\n",
    "                                                         -> Answer\n",
    "                                            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-parametric memory - vector databases\n",
    "\n",
    "The non-parametric memory is usually a dense vector database. Besides being able to store vectors, a vector database must also have the capability to query for the k closest vectors to a given input vector. Like any database, fast storage and querying are important. Ideally, these databases should be able to store data on disk to enable horizontal scaling and data distribution, which are essential for a RAG system to achieve scalability compared to a traditional LLM model. This project will use the Faiss database as it offers all of the above requirements.\n",
    "\n",
    "*The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress, and transform vectors* [2].\n",
    "\n",
    "According to the referenced research, the Faiss database was tested with 768-dimensional ContrieverDb (name of the database from where vectors are imported) dense vector embeddings with up to 1M vectors with 64 bytes per dimension and also with 100M vectors from DeepDb that have a dimension of 96 and 8 bytes per dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence embeding models\n",
    "\n",
    "Sentence embeding models are derived from LLM transformer models. LLMs transformer contains two general components, encoder and decoder. As described in the section \"Generative AI component or a LLM transformer\", the input of an encoder is a text, the output is a contextual embeding. \n",
    "\n",
    "From particular interest for the Sentence embeding models and the RAG system is the encoder layer of the LLMs transformer. \n",
    "\n",
    "Here is a description on how sentences are converted to embedding by an encoder.\n",
    "\n",
    "**LLM Transformer's encoder**\n",
    "```\n",
    "Sentence -> Tokenization -> Embedded Vectors -> Contextual Vectors\n",
    "```\n",
    "\n",
    "The Sentence embeding models incorporates the LLM transformes encoder layer and adds additional layer of *pooling*. This layer operates on the token embedding and groups them in one embeding. This outputs embedding are dependent on the context of the input text, and are called sentence contextual embeding or sentence embeding. All the sentence contextual embedding have the same size, and it is equal to the size of the token embedding.\n",
    "\n",
    "Here is a more visual description on the process of creating sentence embedding by a Sentence embeding model\n",
    "\n",
    "**Sentence embeding model**\n",
    "\n",
    "```\n",
    "Sentence \n",
    " |\n",
    "  -> Tokenization\n",
    "      | \n",
    "       -> Embedded Vectors\n",
    "           | \n",
    "            -> Contextual Vectors\n",
    "                | \n",
    "                 -> | Mean Pooling            |  \n",
    "                    | Max Pooling             | \n",
    "                    | CLS Token               |\n",
    "                    | Attention-based Pooling |\n",
    "                       | \n",
    "                        -> Sentence embeding\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "#### Pooling\n",
    " \n",
    "\n",
    "The pooling phase involves the task of combining multiple contextual token embeddings into one sentence embedding. The dimension of the token embeddings is the same as the created sentence embedding. Here are some of the popular pooling methods used in sentence embedding models:\n",
    "\n",
    "* Mean Pooling\n",
    "* Max Pooling\n",
    "* CLS Token\n",
    "* Attention-based Pooling\n",
    "\n",
    "The most representative method is Mean Pooling, which calculates an embedding using the mean:\n",
    "\n",
    "$$\n",
    "\\text{meanpool}(e_1, e_2, \\ldots, e_n) = \\frac{1}{n} \\sum_{i=1}^{n} e_i\n",
    "$$\n",
    "\n",
    "According to research paper [5], Mean Pooling, Max Pooling, and CLS Token are commonly used techniques.\n",
    "\n",
    "The goal of sentence embedding models is to produce an embedding or vector from text. As mentioned before, they are built by adding an additional layer to the LLM’s transformer encoder component. LLMs produce token embeddings, while sentence embedding models produce embeddings for entire sentences. \n",
    "\n",
    "The requirement of the sentence embedding model is as follows:\n",
    "\n",
    "For every three blocks of text $A$, $P$, $N$, \n",
    "the model $SE$ is to provide three embeddings $SE(A)$, $SE(P)$, $SE(N)$,  \n",
    "such that: \n",
    "if $A$ and $P$ are semanticaly more similar $A$ and $N$, then the norm $|SE(A) - SE(P)|$ < $|SE(A) - SE(N)|$.\n",
    "\n",
    "\n",
    "**The simple interpretation is that the sentence embedding model takes a block of text and captures its meaning or semantics by describing it as a vector. This is an extremely useful feature because it allows for representing the meaning and information of sentences in a way that enables the measurement of semantic information.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese training network and loss function\n",
    " \n",
    "\n",
    "\n",
    "Up to this point, I have explained how a sentence embedding model works, including its input and output. Even though the input and output are in the required format, sentence embedding models are additionally trained to meet the semantic requirements defined above.\n",
    "\n",
    "The main issue during training is that we don't really know the exact output for a block of text, it can be any embeding. It is the relative distance between three embeddings that is important. Semantically close embeddings should be closer than embeddings that have different meanings. To solve this problem, the model is trained using a Siamese network or a variation of the Siamese network, where conceptually three copies of the network model are used, and one single test case consists of three text blocks: $A$, $P$, and $N$. \n",
    "* The text block $A$ is used for comparison and is called an anchor. \n",
    "* The text block $P$ is semantically similar to $A$ and is called the positive case.\n",
    "* The text block $N$ is semantically different from $A$ and $P$ and is called the negative case.\n",
    "\n",
    "\\newpage\n",
    "\n",
    "*Siamese network with triplet loss function*\n",
    "```\n",
    "                   \n",
    "Anchor           Positive           Negative\n",
    " Input             Input            Input\n",
    "   |                 |                |\n",
    "Shared            Shared            Shared\n",
    "Network           Network           Network\n",
    "   |                 |                |\n",
    "Anchor           Positive           Negative\n",
    "Embedding        Embedding          Embedding\n",
    "      \\              |               /\n",
    "       \\             |              /  \n",
    "        \\            |             /\n",
    "         \\           |            /\n",
    "             Triplet Distance\n",
    "               Loss function     \n",
    "```\n",
    "\n",
    "This Siamese network ends with an tripplet distance activation function. The function calcualtes the distance $D(A,P)$ $D(A,N)$ and is activated if $D(A,P)$ > $D(A,N) + \\text{margin}$. The triplet function from the perspective of the Sentence embeder model is a Loss function and its output is used for training.\n",
    "$$\n",
    "Loss = \\max(0, D(A,P) - D(A,N) + \\text{margin})\n",
    "$$\n",
    "[5]\n",
    "\n",
    "If $D(A,P) < D(A,N)$, the function returns 0 and nothing should be adjusted for the current test case. If $D(A,P) > D(A,N)$, then the function returns a value greater than 0, so the model's parameters should be adjusted to minimize this distance. The margin is there to give a bias to the loss function and make sure that only for really small $D(A,P)$ compared to $D(A,N)$ the model should not be adjusted, but for most cases, it will be adjusted. For no adjustments to happen, the following condition should be true: $D(A,P) + \\text{margin} < D(A,N)$.\n",
    "\n",
    "At this point, all of the components of the sentence embedding model are explained. A representative example of a sentence embedding model is SBERT, which is based on the popular LLM BERT. This resource will use SBERT as a Sentence Embedder Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related work\n",
    "\n",
    "### Augmentation \n",
    "\n",
    "Interesting research has been done using the so-called *RAG-Token Model*. The RAG technique described, initially retrieves  n documents related to the question then augments the context of the question and sends the context and the question as an input to an LLM or the generator. This type of RAG system is called the RAG-Sequence Model. The RAG-Token Model retrieves new documents on every generated token. Once a token is generated, the token is appended to all previously generated tokens and this are appended to the initial question. This results in a string of the following form \"Question + GenerateTockenList\". This new string is then used again as an input in the RAG, or as an input for the RAG retrieved. The process retrieves new documents for every new generated token and uses this documents to generate the next token. This makes sure that every new generated token is equally dependent on the question and the non-parametric memory.\n",
    "\n",
    "*The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized*\n",
    "\n",
    "*RAG-Token Model, In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token* [3]\n",
    "\n",
    "### Architecture\n",
    "\n",
    "#### Naive RAG \n",
    "\n",
    "is the type of rag described in this document.\n",
    "\n",
    "#### Advanced RAG \n",
    "\n",
    "Focuses on improving the retrieval of embeddings or data. It adds additional pre-retrieval and post-retrieval phase. In the pre-retrieve several optimization methods are employed like: \n",
    "* query optimization\n",
    "* storage optimization and retrieval\n",
    "* extending the stored data with metadata\n",
    "In the post-retrieval phase idea is to create more usable context from the retrieved data. The data now is reranked and compressed, cleaned up [4]\n",
    "\n",
    "#### Modular RAG \n",
    "\n",
    "This type of architecture tries to split the system in to more modules that can be independently scaled and optimized. Example: Query Processing Module, Retriever, Reranking, Context Management, Generation Module.\n",
    "\n",
    "Also research is done in the field of dense vector databases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research methodology\n",
    "\n",
    "\n",
    "In this research, a simple RAG system will be coded. The system will be flexible and modular enough so can use different LLMS as generators. The system will also have a Retriever that will be able to create embeddings based on sentences and also on paragraphs.\n",
    "Depending on the retriever policy for every LLM three RAG systems will be created:\n",
    "1. The retriever will retrieve one sentence\n",
    "2. The retriever will retrieve two sentences\n",
    "3. The retriever will retrieve one paragraph\n",
    "\n",
    "\n",
    "All of the chosen LLMs will and their appropriate RAG will be asked the same Open-domain question. This is a type of question that can't be answered with simple yes or no and requires accessing a broad range of information to provide an answer and. \n",
    "\n",
    "\n",
    "#### Hardware  \n",
    "\n",
    "For the particular reasons an open sourced LLMs will be used that can be run locally on the pc. \n",
    "The chosen hardware is a m3 macbook with 36GB of RAM.\n",
    "\n",
    "\\newpage\n",
    "#### Retrieval and augmentation \n",
    "\n",
    "There will be 3 types of rag augmentation, in this text called retrieval: \n",
    "\n",
    "* Retrieval of one sentence, will create a context for the question from one sentence using a facts sentence based database.\n",
    "* Retrieval of two sentences, will create a context for the question from two sentence using a facts sentence based database.\n",
    "* Retrieval of one paragraph, will create a context for the question from one paragraph using a facts paragraph based database.\n",
    "\n",
    "\n",
    "The database for the sentences and paragraphs are created from two files containing semantically correct but not necessarily related sentences and paragraphs.\n",
    "\n",
    "\n",
    "#### LLMs\n",
    "\n",
    "Depending on the LLMs training, two types of LLM models will be used: \n",
    "\n",
    "* model for causa LLM, or casual models\n",
    "* model question answering or qa models\n",
    "\n",
    "\n",
    "Regarding the model size, the experiments will be done on small LLMs ranging from  66M up to 1.3B.\n",
    "\n",
    "\n",
    "| Model                                          | Description                             | Number of Parameters | Type                    |\n",
    "|------------------------------------------------|-----------------------------------------|----------------------|-------------------------|\n",
    "| **GPT-2**                                      | OpenAI's Generative Model               | 124M                 | Causal Model            |\n",
    "| **GPT-Neo 125M**                               | EleutherAI's Generative Model           | 125M                 | Causal Model            |\n",
    "| **GPT-Neo 1.3B**                               | EleutherAI's Generative Model           | 1.3B                 | Causal Model            |\n",
    "| **DistilBERT**                                 | Hugging Face's Optimized BERT for QA    | 66M                  | Question Answering Model |\n",
    "| **deepset/roberta-base-squad2**                | Facebook AI's Optimized BERT for QA     | 125M                 | Question Answering Model |\n",
    "| **bert-large-uncased-whole-word-masking-finetuned-squad** | Google's NLU Model for QA    | 340M                 | Question Answering Model |\n",
    "\n",
    "\n",
    "For a comparison, the currently used in production chatGpt 3.5 according to chatGPT has a size of around of 175B. ChatGpt 4.o has not exposed any information about the parameters or architecture. Most probably two models employ RAG technique to improve their performance.\n",
    "\n",
    "\n",
    "For the experiments the LLMs will be grouped in two sets, casual and qa models. It is important to point out the the casual models are not trained for answering questions but for generating text based on input where the question answering models require a question and some context related to the question. The second are more suitable for usage in a rag system but running a rag around casual LLMs will give an idea of how powerful a rag system can be.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### RAG solution\n",
    "The RAG system will be a manual solution, not an existing RAG system. The idea is to show that even a simple RAG system can extend the capabilities of the LLM.\n",
    "\n",
    "\n",
    "#### Sentence embedder\n",
    "for sentence embedder a SBART based embedder will be used `sentence_transformers import SentenceTransformer`\n",
    "\n",
    "\n",
    "#### Nonparametric library\n",
    "For non parametric library `faiss` will be used, it is more than enough to handel this experiment\n",
    "\n",
    "\n",
    "#### Environment \n",
    "Jupyter notebook\n",
    "\n",
    "\n",
    "#### Experiments definition\n",
    "\n",
    "\n",
    "| Experiment | Generators Type         | Retrieval Type | Retrievals |\n",
    "|------------|-------------------------|----------------|------------|\n",
    "| 1          | Causal language models  | Sentence       | 1          |\n",
    "| 2          | Causal language models  | Sentence       | 2          |\n",
    "| 3          | Causal language models  | Paragraph      | 1          |\n",
    "| 4          | QA language models      | Sentence       | 1          |\n",
    "| 5          | QA language models      | Sentence       | 2          |\n",
    "| 6          | QA language models      | Paragraph      | 1          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and discussion\n",
    "The question 'What is the biggest city in Europe?' question is asked on list off llms and rags using different rag retrieval and augmentation techniques.\n",
    "\n",
    "\n",
    "Experiment 1\n",
    "\n",
    "- Generators type: Causal language models\n",
    "- Retrieval type: Sentence\n",
    "- Retreavels : 1\n",
    "\n",
    "\n",
    "| Model                      | LLM Answer                                                        | RAG Answer                        |\n",
    "|----------------------------|-------------------------------------------------------------------|-----------------------------------|\n",
    "| gpt2                       | The biggest city in Europe is Berlin                               | The largest city in Europe is Moscow |\n",
    "| EleutherAI/gpt-neo-125M    | The city of Berlin is the most populous city in the world, and it is the most populous city in the world | The biggest city in Europe is Moscow |\n",
    "| EleutherAI/gpt-neo-1.3B    | The biggest city in Europe is London                               | The answer is: Moscow             |\n",
    "\n",
    "\n",
    "*In this experiment all 3 RAGs answered correctly vs all 3 LLMs answered wrongly*\n",
    "\n",
    "\\newpage\n",
    "Experiment 2\n",
    "\n",
    "- Generators type: Causal language models\n",
    "- Retrieval type: Sentence\n",
    "- Retreavels : 2\n",
    "\n",
    "\n",
    "| Model                      | LLM Answer                                                        | RAG Answer                                              |\n",
    "|----------------------------|-------------------------------------------------------------------|---------------------------------------------------------|\n",
    "| gpt2                       | The biggest city in Europe is Berlin                               | The largest city in Europe is Berlin                    |\n",
    "| EleutherAI/gpt-neo-125M    | The city of Berlin is the most populous city in the world, and it is the most populous city in the world | The city of Moscow is the largest city in Europe       |\n",
    "| EleutherAI/gpt-neo-1.3B    | The biggest city in Europe is London                               | The biggest city in Europe is Berlin, which is the capital of Germany |\n",
    "\n",
    "\n",
    "*Only EleutherAI/gpt-neo-125M answered correctly*\n",
    "\n",
    "\n",
    "Experiment 3\n",
    "\n",
    "- Generators type: Causal language models\n",
    "- Retrieval type: Paragraph\n",
    "- Retreavels : 1\n",
    "\n",
    "\n",
    "| Model                      | LLM Answer                         | RAG Answer                                         |\n",
    "|----------------------------|------------------------------------|----------------------------------------------------|\n",
    "| EleutherAI/gpt-neo-1.3B    | The biggest city in Europe is London | The biggest city in Europe is London                |\n",
    "| gpt2                       | The biggest city in Europe is Berlin | The largest city in Europe is the capital city of Russia |\n",
    "\n",
    "\n",
    "*Only gpt2 answered correctly*\n",
    "\n",
    "\n",
    "Experiment 4\n",
    "\n",
    "- Generators type: QA language models\n",
    "- Retrieval type: Sentence\n",
    "- Retreavels : 1\n",
    "\n",
    "\n",
    "| Model                                          | RAG Answer                                      |\n",
    "|------------------------------------------------|-------------------------------------------------|\n",
    "| distilbert-base-uncased-distilled-squad        | moscow                                          |\n",
    "| deepset/roberta-base-squad2                    | What is the biggest city in Europe? Moscow      |\n",
    "| bert-large-uncased-whole-word-masking-finetuned-squad | moscow                                   |\n",
    "\n",
    "\n",
    "*All models answered correctly*\n",
    "\n",
    "\\newpage\n",
    "Experiment 5\n",
    "\n",
    "- Generators type: QA language models\n",
    "- Retrieval type: Sentece\n",
    "- Retreavels : 2\n",
    "\n",
    "\n",
    "| Model                                          | RAG Answer                                                                                   |\n",
    "|------------------------------------------------|----------------------------------------------------------------------------------------------|\n",
    "| distilbert-base-uncased-distilled-squad        | Moscow is the largest city in Europe by population. Moscow is the capital city of Russia      |\n",
    "| deepset/roberta-base-squad2                    | Moscow                                                                                       |\n",
    "| bert-large-uncased-whole-word-masking-finetuned-squad | Moscow                                                                                       |\n",
    "\n",
    "\n",
    "*all models answered correctly*\n",
    "\n",
    "\n",
    "Experiment 6\n",
    "\n",
    "- Generators type: QA language models\n",
    "- Retrieval type: Paragraph\n",
    "- Retreavels : 1\n",
    "\n",
    "\n",
    "| Model                                          | RAG Answer |\n",
    "|------------------------------------------------|------------|\n",
    "| distilbert-base-uncased-distilled-squad        | Moscow     |\n",
    "| deepset/roberta-base-squad2                    | <s>        |\n",
    "| bert-large-uncased-whole-word-masking-finetuned-squad | Moscow     |\n",
    "\n",
    "\n",
    "**all models answered correctly except deepset/roberta-base-squad2 did not answer**\n",
    "\n",
    "\\newpage\n",
    "#### Casual LLMs Results\n",
    "\n",
    "Experiments 1, 2, and 3 used so-called casual LLMs. Despite low initial expectations, these models performed quite well. Enriched context often influenced their ability to answer questions, and they generally performed similarly. GPT-2 was able to answer questions when provided with an entire paragraph of context but struggled with a two-sentence context. EleutherAI/gpt-neo-125M answered well but has a limited input capacity of 100 characters, making it unsuitable for handling paragraphs or larger documents. EleutherAI/gpt-neo-1.3B did not perform as well, answering correctly in only one experiment. Since the test was limited to one question, it is difficult to determine the most performant model definitively. Nonetheless, it is crucial that the models can answer questions correctly most of the time. These three experiments aim to evaluate whether RAG can produce context capable of influencing the generated text. Given that the models are not specifically trained for question answering and are relatively small, the results are impressive.\n",
    "\n",
    "#### QA LLMs Results\n",
    "\n",
    "Experiments 1, 2, and 3 utilized question answering LLMs. With the exception of deepset/roberta-base-squad2, which failed to produce an answer when the context was a paragraph, all models performed excellently. These models can be considered viable candidates for a production RAG implementation.\n",
    "\n",
    "\n",
    "*As an additional point, the question \"What is the biggest city in Europe?\" perhaps can be changed. The reason is that several models answered London, which might be correct depending on what they consider to be the biggest. The embeddings or the facts stored in the non-parametric model indicate that Moscow has the largest population. Repeating the experiment with a different question that has a more universally agreed-upon answer could help achieve more consistent results across models.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "This text demonstrates through experimentation that Retrieval-Augmented Generation (RAG) systems effectively address some of the most significant challenges faced by Large Language Models (LLMs) and other models with parametric memory. Compared to models solely relying on parametric memory, RAG systems offer several advantages:\n",
    "\n",
    "- **Horizontally Scalable**: RAG systems can efficiently scale by distributing the retrieval process across multiple nodes, allowing for handling larger datasets and more complex queries.\n",
    "- **Distributive**: The modular nature of RAG systems enables distribution of tasks across different components, enhancing robustness and flexibility.\n",
    "- **Mitigates Underfitting and High Bias**: By integrating external knowledge retrieval, RAG systems reduce the need for extremely large models with numerous parameters, thus avoiding issues related to underfitting and high bias.\n",
    "- **Performance Enhancement**: RAG systems can surpass the performance of their base generative models by leveraging external knowledge, resulting in more accurate and contextually relevant responses.\n",
    "\n",
    "As an example, a decent quality question answering system can be created using only a 66M parameter pretrained question answering model like DistilBERT when using an index like FAISS.\n",
    "\n",
    "In summary, RAG systems provide a scalable, distributive, and efficient solution to enhance the capabilities of LLMs, addressing key limitations and improving overall performance without the necessity for excessively large and complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Attention Is All You Need, Ashish Vaswani, Noam ShazeerNiki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin,  https://arxiv.org/pdf/1706.03762 \n",
    "2. The Faiss Library, Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou, https://arxiv.org/pdf/2401.08281\n",
    "3. Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, https://arxiv.org/pdf/2005.11401v4\n",
    "4. Retrieval-Augmented Generation for Large Language Models: A Survey, Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang, https://arxiv.org/pdf/2312.10997\n",
    "5. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, Nils Reimers, Iryna Gurevych, https://arxiv.org/pdf/1908.10084\n",
    "6. https://www.datastax.com/guides/what-is-retrieval-augmented-generation\n",
    "7. https://nexocode.com/blog/posts/retrieval-augmented-generation-rag-llms\n",
    "8. https://implementconsultinggroup.com/article/building-high-quality-rag-systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Rag Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare requered libraries for \n",
    "%conda install -y numpy\n",
    "# Retreavel and storage requered libraries\n",
    "%conda install -y sentence-transformers\n",
    "%conda install -y faiss-cpu\n",
    "# LLMs related libraryes\n",
    "%conda install -y transformers\n",
    "%conda install -y pytorch\n",
    "%conda install -y ipywidgets\n",
    "# %conda install texlive-core\n",
    "%jupyter nbextension uninstall --all\n",
    "%jupyter nbextension install --all\n",
    "%jupyter nbextension enable --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoke test\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LLM related libs\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Retreavel and Storage Related libs\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def read_and_split_sentences(file_path)->list[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    sentences = text.strip().split('.')\n",
    "    return sentences\n",
    "\n",
    "def read_and_split_paragraphs(file_path) -> list[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    paragraphs = text.strip().split('\\n\\n')\n",
    "    return paragraphs\n",
    "\n",
    "def save_dict_to_json_file(data, file_name):\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def get_answers(sentence_retreaver, device, question, llms):\n",
    "    \"\"\"\n",
    "    Generates answers using different LLMs and returns the results.\n",
    "\n",
    "    :param sentence_retreaver: Instance of Retreaver with added documents\n",
    "    :param device: Torch device (e.g., 'cpu', 'cuda', 'mps')\n",
    "    :param question: The question to be answered\n",
    "    :param llms: List of LLMs to generate answers\n",
    "    :return: List of results with answers from each LLM\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for llm in llms:\n",
    "        generator = Generator(model_name_in=llm, device_in=device)\n",
    "        single_result = {}\n",
    "        single_result[\"llm\"] = llm\n",
    "        single_result[\"question\"] = question\n",
    "        single_result[\"LLM Answer\"] = generator.generate_answer(question)\n",
    "        single_result[\"RAG Answer\"] = generator.generate_answer(question, context=sentence_retreaver.retrieve_documents(question, 2))\n",
    "        results.append(single_result)\n",
    "    \n",
    "    return results    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    # Causal language models\n",
    "    # gpt_j_6B = \"EleutherAI/gpt-j-6B\"\n",
    "    gpt_2 = \"gpt2\"\n",
    "    gpt_neo_125M = \"EleutherAI/gpt-neo-125M\"\n",
    "    gpt_neo_1_3B = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    # Question answering models\n",
    "    distilbert = \"distilbert-base-uncased-distilled-squad\"\n",
    "    roberta = \"deepset/roberta-base-squad2\"\n",
    "    bert = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "    casual_llms = [gpt_2, gpt_neo_125M, gpt_neo_1_3B]\n",
    "    qa_llms = [distilbert, roberta, bert]\n",
    "\n",
    "\n",
    "    def __init__(self, model_name_in: str, device_in):\n",
    "        self.device = device_in\n",
    "        self.model_name = model_name_in\n",
    "        self.model, self.tokenizer = self.get_model(self.model_name, self.device)\n",
    "\n",
    "    def get_model(self, model_name_in: str, device_in):\n",
    "        if model_name_in in self.casual_llms:\n",
    "            _model = AutoModelForCausalLM.from_pretrained(model_name_in).to(device_in)\n",
    "        else:\n",
    "            _model = AutoModelForQuestionAnswering.from_pretrained(model_name_in).to(device_in)\n",
    "        _tokenizer = AutoTokenizer.from_pretrained(model_name_in)\n",
    "        return _model, _tokenizer\n",
    "\n",
    "    def generate_answer(self, question: str, context=None):\n",
    "        if self.model_name in self.casual_llms:\n",
    "            return self.generate_answer_causal(question, context)\n",
    "        else:\n",
    "            return self.generate_answer_qa(question, context)\n",
    "\n",
    "    def generate_answer_causal(self, question: str, context_in=None, max_length=100):\n",
    "        _context = None\n",
    "        if isinstance(context_in, list):\n",
    "            _context = \" and \".join(context_in)\n",
    "            _context = _context.lstrip()\n",
    "            _context = _context.rstrip()\n",
    "            if(not _context.endswith((',','.','?','!'))):\n",
    "                _context = _context + '.'\n",
    "                \n",
    "        _input_text = question if not context_in else \"Given, \" + _context + \" \" + question +\"\"\n",
    "        _inputs = self.tokenizer.encode_plus(_input_text, return_tensors=\"pt\")\n",
    "        _inputs = _inputs.to(self.device) \n",
    "        # Disable gradient calculation for faster inference\n",
    "        with torch.no_grad():\n",
    "            _outputs = self.model.generate(\n",
    "                input_ids=_inputs['input_ids'], \n",
    "                attention_mask=_inputs.get('attention_mask'), \n",
    "                max_length=max_length,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        _response = self.tokenizer.decode(_outputs[0][_inputs['input_ids'].size(1):], skip_special_tokens=False)\n",
    "        _response = _response.lstrip('\\n')\n",
    "        _response = _response.lstrip('A:')\n",
    "        _response_array = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *',_response.replace('\\n',' ').strip())\n",
    "        _one_sentece_response =_response_array[0]\n",
    "        return _one_sentece_response\n",
    "\n",
    "    def generate_answer_qa(self, question_in: str, context_in=None):\n",
    "        if context_in is None:\n",
    "            context_in = \"\"\n",
    "        if isinstance(context_in, list): \n",
    "            context_in = \" \".join(context_in)\n",
    "        inputs = self.tokenizer.encode_plus(question_in, context_in, add_special_tokens=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        answer_start = torch.argmax(outputs.start_logits)  # Get the most likely beginning of answer\n",
    "        answer_end = torch.argmax(outputs.end_logits) + 1  # Get the most likely end of answer\n",
    "        \n",
    "        answer = self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retreaver:\n",
    "    def __init__(self, model_name_in='all-MiniLM-L6-v2'):\n",
    "        self.documents = []\n",
    "        self.model = SentenceTransformer(model_name_in)\n",
    "        self.document_embeddings = None\n",
    "        # dimension is 384 for the 'all-MiniLM-L6-v2'\n",
    "        d = self.model.get_sentence_embedding_dimension()\n",
    "        self.index = faiss.IndexFlatL2(d)\n",
    "        \n",
    "    def add_documents(self, new_documents_in: list[str]):\n",
    "        # MaximumCharacters ≈ 512tokens×4characters/token=2048characters\n",
    "        new_document_embeddings = self.model.encode(new_documents_in).astype(np.float32)\n",
    "        self.index.add(new_document_embeddings)\n",
    "        self.documents.extend(new_documents_in)\n",
    "\n",
    "    def retrieve_documents(self, query_in, results_size_in=1):\n",
    "        query_embedding = self.model.encode([query_in]).astype(np.float32)\n",
    "        distances, indices = self.index.search(query_embedding, results_size_in)\n",
    "        retrieved_documents = [self.documents[idx] for idx in indices[0]]\n",
    "        return retrieved_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Experiment 1\n",
    "# Simple Question experiment \n",
    "# Generators type: Causal language models\n",
    "# Retreavel type: Sentence retreavel\n",
    "# Retreavels : 1\n",
    "\n",
    "sentence_retreaver = Retreaver()\n",
    "sentence_retreaver.add_documents(read_and_split_sentences(\"facts_sentences\"))\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "question = \"What is the biggest city in Europe?\"\n",
    "results = []\n",
    "\n",
    "for llm in Generator.casual_llms:\n",
    "    generator = Generator(model_name_in=llm, device_in=device)\n",
    "    single_result = {}\n",
    "    single_result[\"llm\"] = llm\n",
    "    single_result[\"question\"] = question\n",
    "    single_result[\"LLM Answer\"] = generator.generate_answer(question)\n",
    "    single_result[\"RAG Answer\"] = generator.generate_answer(question, context = sentence_retreaver.retrieve_documents(question,1))\n",
    "    results.append(single_result)\n",
    "\n",
    "save_dict_to_json_file(results, \"experiments/CasualModelsSimpleQuestion1SentenceRetreavel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Experiment 2\n",
    "# Simple Question experiment \n",
    "# Generators type: Causal language models\n",
    "# Retreavel type: Sentence\n",
    "# Retreavels : 2\n",
    "\n",
    "sentence_retreaver = Retreaver()\n",
    "sentence_retreaver.add_documents(read_and_split_sentences(\"facts_sentences\"))\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "question = \"What is the biggest city in Europe?\"\n",
    "results = []\n",
    "\n",
    "for llm in Generator.casual_llms:\n",
    "    generator = Generator(model_name_in=llm, device_in=device)\n",
    "    single_result = {}\n",
    "    single_result[\"llm\"] = llm\n",
    "    single_result[\"question\"] = question\n",
    "    single_result[\"LLM Answer\"] = generator.generate_answer(question)\n",
    "    single_result[\"RAG Answer\"] = generator.generate_answer(question, context = sentence_retreaver.retrieve_documents(question,2))\n",
    "    results.append(single_result)\n",
    "\n",
    "save_dict_to_json_file(results, \"experiments/CasualModelsSimpleQuestion2SentenceRetreavel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Experiment 3\n",
    "# Simple Question experiment \n",
    "# Generators type: Causal language models\n",
    "# Retreavel type: Paragraph\n",
    "# Retreavels : 1\n",
    "\n",
    "sentence_retreaver = Retreaver()\n",
    "sentence_retreaver.add_documents(read_and_split_paragraphs(\"facts_paragraphs\"))\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "question = \"What is the biggest city in Europe?\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for llm in [Generator.gpt_neo_1_3B, Generator.gpt_2]:\n",
    "    print(f\"Processing {llm}\")\n",
    "    generator = Generator(model_name_in=llm, device_in=device)\n",
    "    single_result = {}\n",
    "    single_result[\"llm\"] = llm\n",
    "    single_result[\"question\"] = question\n",
    "    single_result[\"LLM Answer\"] = generator.generate_answer(question)\n",
    "    single_result[\"RAG Answer\"] = generator.generate_answer(question, context = sentence_retreaver.retrieve_documents(question,1))\n",
    "    results.append(single_result)\n",
    "\n",
    "\n",
    "save_dict_to_json_file(results, \"experiments/CasualModelsSimpleQuestionParagraphRetreavel.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Experiment 4\n",
    "# Simple Question experiment \n",
    "# Generators type: QA language models\n",
    "# Retreavel type: Sentence\n",
    "# Retreavels : 1\n",
    "\n",
    "sentence_retreaver = Retreaver()\n",
    "sentence_retreaver.add_documents(read_and_split_sentences(\"facts_sentences\"))\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "question = \"What is the biggest city in Europe?\"\n",
    "\n",
    "results = []\n",
    "for llm in Generator.qa_llms:\n",
    "    generator = Generator(model_name_in=llm, device_in=device)\n",
    "    single_result = {}\n",
    "    single_result[\"llm\"] = llm\n",
    "    single_result[\"question\"] = question\n",
    "    single_result[\"RAG Answer\"] = generator.generate_answer(question, context = sentence_retreaver.retrieve_documents(question,1))\n",
    "    results.append(single_result)\n",
    "\n",
    "save_dict_to_json_file(results, \"experiments/QAModelsSimpleQuestionOneSentenceRetreavel.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Experiment 5\n",
    "# Simple Question experiment \n",
    "# Generators type: QA language models\n",
    "# Retreavel type: Sentence\n",
    "# Retreavels : 2\n",
    "\n",
    "sentence_retreaver = Retreaver()\n",
    "sentence_retreaver.add_documents(read_and_split_sentences(\"facts_sentences\"))\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "question = \"What is the biggest city in Europe?\"\n",
    "\n",
    "results = []\n",
    "for llm in Generator.qa_llms:\n",
    "    generator = Generator(model_name_in=llm, device_in=device)\n",
    "    single_result = {}\n",
    "    single_result[\"llm\"] = llm\n",
    "    single_result[\"question\"] = question\n",
    "    single_result[\"RAG Answer\"] = generator.generate_answer(question, context = sentence_retreaver.retrieve_documents(question,2))\n",
    "    results.append(single_result)\n",
    "\n",
    "save_dict_to_json_file(results, \"experiments/QAModelsSimpleQuestionTwoSentenceRetreavel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Experiment 6\n",
    "# Simple Question experiment \n",
    "# Generators type: QA language models\n",
    "# Retreavel type: Paragraph\n",
    "# Retreavels : 1\n",
    "\n",
    "sentence_retreaver = Retreaver()\n",
    "sentence_retreaver.add_documents(read_and_split_paragraphs(\"facts_paragraphs\"))\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "question = \"What is the biggest city in Europe?\"\n",
    "\n",
    "results = []\n",
    "for llm in Generator.qa_llms:\n",
    "    generator = Generator(model_name_in=llm, device_in=device)\n",
    "    single_result = {}\n",
    "    single_result[\"llm\"] = llm\n",
    "    single_result[\"question\"] = question\n",
    "    single_result[\"RAG Answer\"] = generator.generate_answer(question, context = sentence_retreaver.retrieve_documents(question,1))\n",
    "    results.append(single_result)\n",
    "\n",
    "save_dict_to_json_file(results, \"experiments/QAModelsSimpleQuestionParagraphRetreavel.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- \n",
    "# Terms\n",
    "* Generative AI -Generative AI refers to artificial intelligence technologies that create new content, ranging from text and images to code, by learning patterns from existing data. `https://www.datastax.com/guides/what-is-generative-ai`\n",
    "\n",
    "\n",
    "## Papers\n",
    "* https://arxiv.org/abs/2005.11401v4 facebook on rag initial document *\n",
    "\n",
    "\n",
    "## Links\n",
    "* https://www.datastax.com/guides/what-is-retrieval-augmented-generation\n",
    "* https://ar5iv.labs.arxiv.org/html/2312.10997 - academic\n",
    "* https://nexocode.com/blog/posts/retrieval-augmented-generation-rag-llms/\n",
    "* [https://www.datastax.com/guides/what-is-retrieval-augmented-generation](https://www.datastax.com/guides/what-is-retrieval-augmented-generation)\n",
    "* https://ar5iv.labs.arxiv.org/html/2404.07220v1 - academic\n",
    "* https://implementconsultinggroup.com/article/building-high-quality-rag-systems\n",
    "\n",
    "<!-- https://www.langchain.com/ -->\n",
    "<!-- \n",
    "Todo \n",
    "* Denced vectors - A dense vector index is a data structure that stores these dense vectors for efficient retrieval and similarity search. Here’s how it works in the context of Wikipedia:\n",
    "\n",
    "* We compare two RAG formulations, one which conditions on the same retrieved passages\n",
    "across the whole generated sequence, and another which can use different passages\n",
    "per token\n",
    "\n",
    "* tip na prasana\n",
    "    * open-domain question answering [5, 29],\n",
    "    * fact checking [56], \n",
    "    * fact completion [48]\n",
    "    * long-form question answering [12] -->\n",
    "  <!-- * Wikipedia articlegeneration [36], \n",
    "    * dialogue [41, 65, 9, 13], \n",
    "    * translation [17] \n",
    "    * language modeling [19, 27]\n",
    "\n",
    "* bias varianse -->\n",
    "\n",
    "<!-- \n",
    "Rag system:\n",
    "* We introduce RAG models where the parametric\n",
    "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
    "vector index of Wikipedia\n",
    "\n",
    "* useful links reaserch papers\n",
    "1. https://arxiv.org/pdf/2005.11401v4 rag experiment\n",
    "2. https://arxiv.org/pdf/2312.10997 rag clasification\n",
    "3. https://ar5iv.labs.arxiv.org/html/2404.07220v1 - zboruva za 'Blended Retrievers' ili ’Blended RAG' Dense Vector indexes and Sparse Encoder indexes, definirajgi poimite i tuka ima tabela za retreavcel performance na tocnost\n",
    "4. https://arxiv.org/abs/2401.08281 fass library\n",
    "\n",
    "LLM Transformer\n",
    "Sentence -> Tokenization -> Embedded Vectors -> Contextual Vectors\n",
    "\n",
    "Sentence Embedding Models\n",
    "Sentence -> Tokenization -> Embedded Vectors -> Contextual Vectors -> (Mean Pooling,  Max Pooling, CLS Token, Attention-based Pooling)\n",
    " -> Sentence Vector\n",
    "Mean Pooling calculates the average vector for every sentence -->\n",
    "<!-- \n",
    "* SBERT: Utilizes siamese and triplet network structures for efficient and meaningful sentence embeddings.\n",
    "* Dimensionality: Optimal dimensionality is crucial for retaining information in embeddings; strategies to minimize performance loss involve focusing on both encoder and pooler components.\n",
    "* Pooling Methods: Various pooling methods (CLS token, mean pooling, max pooling) affect the quality of sentence embeddings differently.\n",
    "* In-Context Learning: Large language models benefit from in-context learning to generate better sentence embeddings.\n",
    "\n",
    "1. Sentence-BERT (SBERT)\n",
    "The paper \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\" by Reimers and Gurevych (2019) introduces SBERT, which modifies the BERT network to create semantically meaningful sentence embeddings. SBERT employs a siamese network structure, allowing it to compute sentence embeddings that can be efficiently compared using cosine similarity. This structure enables SBERT to outperform traditional BERT in tasks like Semantic Textual Similarity (STS) by using mean pooling over token embeddings and fine-tuning on sentence pairs with a triplet loss objective​ (ar5iv)​. https://ar5iv.labs.arxiv.org/html/1908.10084\n",
    "\n",
    "2. Dimensionality of Sentence Embeddings\n",
    "The paper \"On the Dimensionality of Sentence Embeddings\" provides an empirical analysis of how the dimensionality of sentence embeddings affects performance across various tasks. It demonstrates that reducing the dimensionality of embeddings can lead to performance loss, not just due to the reduced dimension but also due to a decrease in the quality of the encoder's output. This study offers strategies for mitigating performance loss by separately addressing the encoder and pooler components​ (ar5iv)​.https://ar5iv.labs.arxiv.org/html/2310.15285\n",
    "\n",
    "3. Evaluation of BERT and ALBERT Sentence Embeddings\n",
    "In \"Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks,\" the authors evaluate the effectiveness of BERT and ALBERT models in generating sentence embeddings for tasks like Semantic Textual Similarity (STS) and Natural Language Inference (NLI). The study compares different pooling strategies (CLS token, average pooling, max pooling) and highlights the effectiveness of SBERT and SALBERT, which use siamese and triplet network structures for enhanced performance​ (ar5iv)​. https://ar5iv.labs.arxiv.org/html/2101.10642v1\n",
    "\n",
    "4. Scaling Sentence Embeddings with Large Language Models\n",
    "The paper \"Scaling Sentence Embeddings with Large Language Models\" explores methods to enhance sentence embeddings using large language models (LLMs) like OPT. The study introduces a prompt-based method with explicit one-word limitations to improve sentence representation. It shows that in-context learning can significantly enhance the quality of sentence embeddings, especially as the model size increases​ (ar5iv)​. https://ar5iv.labs.arxiv.org/html/2307.16645 -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
