{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic\n",
    "Extending the capabilities of LLMs using Retrieval-Augmented Generation techniques. Comparing the accuracy of answering questions between standard LLM vs. RAG LLM for different LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "This research aims to enhance the capabilities of Large Language Models (LLMs) using Retrieval-Augmented Generation (RAG) techniques. By comparing the accuracy of question-answering between standard LLMs and those integrated with RAG systems, this study explores the potential improvements in performance and robustness that RAG systems can offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "LLMs are quite useful and have taken the world of AI by storm. They have extensive usages, and we are still rediscovering what they can be used for. However, there are some aspects that are not ideal, for example:\n",
    "\n",
    "- **Limitation in Memory**: LLMs are trained once on a set of data. Once trained, they cannot answer questions or generate text on data they have not been trained on (problem of under-fitting and high bias).\n",
    "- **Hardware Requirements and Cost**: Running useful general-purpose LLM requires serious hardware capabilities, which comes with additional costs.\n",
    "- **Privacy**: Large and useful LLMs are typically run by companies, requiring users to send data to externally managed servers. This may pose problems for individual users who need to submit private data and for companies, especially with GDPR requirements that might not be satisfied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Retrieval augmented generation\n",
    "\n",
    "\n",
    "*Retrieval-augmented generation (RAG) is an advanced artificial intelligence (AI) technique that combines information retrieval with text generation, allowing AI models to retrieve relevant information from a knowledge source and incorporate it into generated text.*[6]\n",
    "\n",
    "\n",
    "The basic idea around a RAG system is extending the capabilities of an already trained machine learning model (MLM) in particular an LLM. \n",
    "\n",
    "One common problem in LLMs is that in order get a correct output/answer the LLM needs to be trained on a dataset that is relevant to the input/question. For example, asking an LLM who is the winner of 2024 euro championship will not result in a correct answer if the model is trained before 2024. Regardless of the capabilities of the chosen LLM, it just does not contain memory related to this information. In case like this the LLM will need to be retrained. Different problem that occurs in LLMs and machine models in general is that perhaps the LLM is not powerful enough for the given task. Example for the second case is that if a model is asked extremely difficult question, even though the model has been trained on all the relevant data it might still not produce correct output in case of a difficult task.\n",
    "\n",
    " Rag systems do not suffer as much as other models from this problem, their memory can be extended without the need for training or adding additional parameters. \n",
    " The system has two types of memory:\n",
    " \n",
    "- **Parametric memory** - refers to the knowledge in the LLM generative model \n",
    "- **Non-parametric memory** - stored in special types of databases. \n",
    "This non-parametric memory can be extended with additional knowledge easily and it still can be used by the LLM. \n",
    "**This is the core idea in RAG. Without any additional training new knowledge can be made available to the LLM.**\n",
    "\n",
    "\n",
    "The RAG technique integrates a retrieval model and generative model. RAG systems usually work around extending the capabilities of NLP(Natural Language processing models) giving them additional memory or information on various topics. Therefor a production RAG system in the context of LLMs is build from 2 sub-systems:\n",
    "\n",
    "\n",
    "* **Retrieval and Augmentation**\n",
    "* **Generative AI component** or parametric memory, it is usually language generation tool like LLMs\n",
    "\n",
    "\n",
    "From the perspective of usage of the RAG system, generating an answer is done in three phases:\n",
    "\n",
    "- Phase 1 Retrieval, Knowledge retrieval, When asked a question the RAG retrieval system will retrieve all known information by submitting an appropriate query to the non-parametric memory.\n",
    "- Phase 2 Augmentation, with the retrieved information, a context around the question is created, the context should contain all information for answering the question.\n",
    "- Phase 3 Generation, The context and question will be passed to an LLM that will generate an answer.\n",
    "\n",
    "```\n",
    "Question  \n",
    "    |\n",
    "     ---> | Retrieval and | \n",
    "          | Augmentation  |\n",
    "                 |\n",
    "                  --> (context + question)\n",
    "                               |\n",
    "                                ---> | Generative |\n",
    "                                     |     AI     |\n",
    "                                     |  Component  | --> Answer             \n",
    "              \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "# Generative AI component or a LLM transformer\n",
    "\n",
    "\n",
    "The Generative AI component is an LLM transformer. Typically, the transformer has two main components, the encoder and the decoder. The input text is initially tokenized, meaning it is split into small continuous lists of characters. In the next step, the model transforms the tokens into fixed-size vectors called embeddings. In the final step, in a so-called attention layer, an additional linear transformation on the embeddings is performed, making them dependent on the context they appear in. These context-dependent embeddings, also called contextual embeddings, are the output of the encoder and the input to the decoder. The encoder is of extreme importance in this text as it is a crucial part of a RAG system.\n",
    "\n",
    "\n",
    "*Embeddings are vector representations, typically produced by a neural network, whose main objective is to map (embed) the input media item into a vector space, where the locality encodes the semantics of the input* [2].\n",
    "\n",
    "\n",
    "The decoder takes the input and generates new embeddings and again combines the output with the embedded tokens to produce new tokens. The vector transformation where token embeddings are transformed to different ones to capture the context of the input text is called attention. It is described in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762):\n",
    "As a result of the initial research paper on transformers:\n",
    "The LLM transformers do not suffer from maintaining context and dependencies as much as recurrent or convolutional networks.\n",
    "\n",
    "\n",
    "*Transformers can be trained significantly faster than architectures based on recurrent or convolutional layers.*\n",
    "*Transformers outperform the best previously reported models in translation.* [1]\n",
    "\n",
    "\n",
    "The quality of the generated text generally depends on the number of the parameters of the model.\n",
    "Regardless of the quality of the output, every LLM for a given input is expected to generate grammatically correct text and semantically related to the input. \n",
    "For example if I ask even the smallest LLM \"What is the capital of France?\" it might return \"Berlin is the capital of France\". \n",
    "The answers will perhaps not be correct, but they will contain the context of the question and grammatically correct.\n",
    "\n",
    "\n",
    "**The generative component of the RAG system needs to be able for a given input and context to generate grammatically correct sentences related to the given context.**\n",
    "\n",
    "**This is exactly what is achieved with the LLM transformers and that is why they are ideal to be used in the generation phase of the RAG system.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "# Retrieval System\n",
    "\n",
    "The retrieval system is used to store and retrieve knowledge outside the knowledge maintained in the LLM. The knowledge here is in the form of paragraphs, sentences, or other forms of continuous text, referred to here as a text block. The text blocks are stored as embeddings.\n",
    "\n",
    "This part of the system is composed of two components or modules:\n",
    "\n",
    "* Sentence Embedding Model\n",
    "* Non-parametric memory\n",
    "\n",
    "\n",
    "## Storing Phase:\n",
    "The idea of the retrieval phase is for a given text block to create embedding and store this embedding in the non-parametric memory.\n",
    "\n",
    "The process is as follows: \n",
    "\n",
    "1. Take a text block, sentence, or paragraph.\n",
    "2. Passes this text to the Sentence Embedding model to get an embedding(dense vector).\n",
    "3. Stores this embedding in a non-parametric memory.\n",
    "\n",
    "```\n",
    "Sentence\n",
    "|\n",
    " -> Tokenization\n",
    "     |\n",
    "      -> Embedded Vectors\n",
    "          |\n",
    "           -> Contextual Vectors\n",
    "```\n",
    "\\newpage\n",
    "\n",
    "## Retrieval Phase:\n",
    "The idea of the retrieval phase is to return all relevant information from the non-parametric memory for a given text, query, or question.\n",
    "\n",
    "The process is as follows: \n",
    "\n",
    "1. Takes input text, referred to as a query.\n",
    "2. Passes the query text to the Sentence Embedding model to get a query embedding or a query dense vector.\n",
    "3. Using the query embedding, fetches a list of related knowledge from the non-parametric memory.\n",
    "\n",
    "```\n",
    "Question \n",
    "   |\n",
    "    -> Sentence Embedder \n",
    "   |            | \n",
    "   |             -> Embedding \n",
    "   |                 |\n",
    "   |                  -> Non-Parametric memory \n",
    "   |                         |\n",
    "   |                          -> Retrieved Documents\n",
    "   |                                   |\n",
    "   |                                    -> Context\n",
    "   |                                          | \n",
    "   |                                           -->| Generative |\n",
    "    --------------------------------------------->| Component  |\n",
    "                                                        |\n",
    "                                                         -> Answer\n",
    "                                            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-parametric memory - vector databases\n",
    "\n",
    "The non-parametric memory is usually a dense vector database. Besides being able to store vectors, a vector database must also have the capability to query for the k the closest vectors to a given input vector. Like any database, fast storage and querying are important. Ideally, these databases should be able to store data on disk to enable horizontal scaling and data distribution, which are essential for a RAG system to achieve scalability compared to a traditional LLM model. This project will use the Faiss library, as it sutisfies the conditions for vector storage and quering the k closest vectors, it laks the automatic distributive scaling but a full DB solution is not required for the project in this paper.\n",
    "\n",
    "*The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress, and transform vectors* [2].\n",
    "\n",
    "According to the referenced research, the Faiss database was tested with 768-dimensional ContrieverDb (name of the database from where vectors are imported) dense vector embeddings with up to 1M vectors with 64 bytes per dimension and also with 100M vectors from DeepDb that have a dimension of 96 and 8 bytes per dimension.\n",
    "\n",
    "#### Popular vector databases\n",
    "In recent years, there have been significant developments in the field of vector databases. Several new dense vector databases have emerged, and some older databases, like NodeDB and MongoDB, have begun incorporating vector capabilities.\n",
    "\n",
    "Currently, (as of 2024), these are the options for dense vector storage:\n",
    "\n",
    "* Redis Vector Similarity Search (VSS)\n",
    "* Pinecone - exclusively managed, closed source\n",
    "* Weaviate - open source\n",
    "* Milvus - open source\n",
    "* Qdrant - open source\n",
    "* Vespa - open source\n",
    "* Cloudflare Vectorize - exclusively managed, closed source\n",
    "* MongoDB Atlas - fully managed, closed source\n",
    "* Postges, pgvector - open-sourced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence embedding models\n",
    "\n",
    "Sentence embedding models are derived from LLM transformer models. LLMs transformer contains two general components, encoder and decoder. As described in the section \"Generative AI component or a LLM transformer\", the input of an encoder is a text, the output is a contextual embedding. \n",
    "\n",
    "From particular interest for the Sentence embedding models and the RAG system is the encoder layer of the LLMs transformer. \n",
    "\n",
    "Here is a description on how sentences are converted to embedding by an encoder.\n",
    "\n",
    "**LLM Transformer's encoder**\n",
    "```\n",
    "Sentence\n",
    "|\n",
    " -> Tokenization\n",
    "     |\n",
    "      -> Embedded Vectors\n",
    "          |\n",
    "           -> Contextual Vectors\n",
    "```\n",
    "\n",
    "The Sentence embedding models incorporates the LLM transformers encoder layer and adds additional layer of *pooling*. This layer operates on the token embedding and groups them in one embedding. This outputs embedding are dependent on the context of the input text, and are called sentence contextual embedding or sentence embedding. All the sentence contextual embedding have the same size, and it is equal to the size of the token embedding.\n",
    "\n",
    "Here is a more visual description on the process of creating sentence embedding by a Sentence embedding model\n",
    "\n",
    "**Sentence embedding model**\n",
    "\n",
    "```\n",
    "Sentence \n",
    " |\n",
    "  -> Tokenization\n",
    "      | \n",
    "       -> Embedded Vectors\n",
    "           | \n",
    "            -> Contextual Vectors\n",
    "                | \n",
    "                 -> | Mean Pooling            |  \n",
    "                    | Max Pooling             | \n",
    "                    | CLS Token               |\n",
    "                    | Attention-based Pooling |\n",
    "                       | \n",
    "                        -> Sentence embedding\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "#### Pooling\n",
    " \n",
    "\n",
    "The pooling phase involves the task of combining multiple contextual token embeddings into one sentence embedding. The dimension of the token embeddings is the same as the created sentence embedding. Here are some of the popular pooling methods used in sentence embedding models:\n",
    "\n",
    "* Mean Pooling\n",
    "* Max Pooling\n",
    "* CLS Token\n",
    "* Attention-based Pooling\n",
    "\n",
    "The most representative method is Mean Pooling, which calculates an embedding using the mean:\n",
    "\n",
    "$$\n",
    "\\text{meanpool}(e_1, e_2, \\ldots, e_n) = \\frac{1}{n} \\sum_{i=1}^{n} e_i\n",
    "$$\n",
    "\n",
    "According to research paper [5], Mean Pooling, Max Pooling, and CLS Token are commonly used techniques.\n",
    "\n",
    "The goal of sentence embedding models is to produce an embedding or vector from text. As mentioned before, they are built by adding a layer to the LLM’s transformer encoder component. LLMs produce token embeddings, while sentence embedding models produce embeddings for entire sentences. \n",
    "\n",
    "The requirement of the sentence embedding model is as follows:\n",
    "\n",
    "For every three blocks of text $A$, $P$, $N$, \n",
    "the model $SE$ is to provide three embeddings $SE(A)$, $SE(P)$, $SE(N)$,  \n",
    "such that: \n",
    "if $A$ and $P$ are semantically more similar $A$ and $N$, then the distance $distance(SE(A), SE(P))$ < $distance(SE(A), SE(N))$.\n",
    "\n",
    "\n",
    "**The simple interpretation is that the sentence embedding model takes a block of text and captures its meaning or semantics by describing it as a vector. This is an extremely useful feature because it allows for representing the meaning and information of sentences in a way that enables the measurement of semantic information.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\newpage\n",
    "\n",
    "#### Siamese training network and loss functions\n",
    " \n",
    "\n",
    "Up to this point, I have explained how a sentence embedding model works, including its input and output. Even though the input and output are in the required format, sentence embedding models are additionally trained to meet the semantic requirements defined above.\n",
    "\n",
    "The main issue during training is that we don't really know the exact output for a block of text, it can be any embedding. It is the relative distance between the embeddings that is important. Semantically close embeddings should be closer than embeddings that have different meanings. To solve this problem, the model is trained using a Siamese network or a variation of the Siamese network. Initially the sentence embedder is trained with Siamese network that conceptual contains two copies of the same neural network. These two networks share the same parameters. The network ends with a loss function. Popular lost functions are contrastive loss and the triplet loss function. \n",
    "\n",
    "##### Contrastive loss function\n",
    "\n",
    "Contrastive loss function uses the Siamese network. The training set elements consist of two text blocks $A$, $C$ where:\n",
    "\n",
    "* $A$ - is called an anchor case, it is used to compare against\n",
    "* $C$ - means positive or negative case, is either a text similar to $A$ or a text block that is not similar to $A$\n",
    "\n",
    "*Siamese network with Contrastive Loss function*\n",
    "```\n",
    " Anchor                        Positive/Negative\n",
    "  Input                            Input\n",
    "    |                                |\n",
    " Shared Network                Shared Network \n",
    "    |                                |\n",
    " Anchor Embedding          Positive/Negative Embedding\n",
    "       \\                         /\n",
    "        \\                       /\n",
    "         \\                     /\n",
    "           Contrastive Loss\n",
    "                Function\n",
    "\n",
    "```\n",
    "The networks end's with a contrastive loss function that is given with the following formula:\n",
    "$$\n",
    "Loss =  \\frac{1}{2} (y*D(A,C)^2 + (1-y)*max(0,m-D(A,C))^2)\n",
    "$$\n",
    "\n",
    "In the previous function $y$ can be 1 or 0. This is because the network can be trained on cases with similar and dissimilar embeddings.\n",
    "\n",
    "* Similar embeddings (y=1), the form of the loss function is $Loss = \\frac{1}{2}(D(A,C))$ \n",
    "* Dissimilar embeddings (y=0), the form of the loss function is $Loss =  \\frac{1}{2}(max(0,m-D(A,C))^2)$. In this case $m$ acts as a kind of tolerance level and the function is triggered only for a distance bigger than $m$.\n",
    "\n",
    "\n",
    "\\newpage\n",
    "\n",
    "##### Triplet Loss function\n",
    "Similarly to the Contrastive loss function training method, the triplet loss function uses a variation of the Siamese network where instead of two networks, three networks are used. One single test case consists of three text blocks: $A$, $P$, and $N$. \n",
    "* The text block $A$ is used for comparison and is called an anchor. \n",
    "* The text block $P$ is semantically similar to $A$ and is called the positive case.\n",
    "* The text block $N$ is semantically different from $A$ and $P$ and is called the negative case.\n",
    "\n",
    "*Siamese network with triplet loss function*\n",
    "```\n",
    "                   \n",
    "Anchor           Positive           Negative\n",
    " Input             Input            Input\n",
    "   |                 |                |\n",
    "Shared            Shared            Shared\n",
    "Network           Network           Network\n",
    "   |                 |                |\n",
    "Anchor           Positive           Negative\n",
    "Embedding        Embedding          Embedding\n",
    "      \\              |               /\n",
    "       \\             |              /  \n",
    "        \\            |             /\n",
    "         \\           |            /\n",
    "             Triplet Distance\n",
    "               Loss function     \n",
    "```\n",
    "\n",
    "This Siamese network ends with a triplet distance activation function. The function calculates the distance $D(A,P)$ $D(A,N)$ and is activated if $D(A,P)$ > $D(A,N) + \\text{margin}$. The triplet function from the perspective of the Sentence embedder model is a Loss function and its output is used for training.\n",
    "$$\n",
    "Loss = \\max(0, D(A,P) - D(A,N) + \\text{margin})\n",
    "$$\n",
    "[5]\n",
    "\n",
    "If $D(A,P) < D(A,N)$, the function returns 0 and nothing should be adjusted for the current test case. If $D(A,P) > D(A,N)$, then the function returns a value greater than 0, so the model's parameters should be adjusted to minimize this distance. The margin is there to give a bias to the loss function and make sure that only for really small $D(A,P)$ compared to $D(A,N)$ the model should not be adjusted, but for most cases, it will be adjusted. For no adjustments to happen, the following condition should be true: $D(A,P) + \\text{margin} < D(A,N)$.\n",
    "\n",
    "#####  Contrastive vs Triplet Loss function\n",
    "Both loss functions are used for training the embedder to detect semantic similarities. The primary difference is that contrastive loss is easier to implement in the context of generating the training set. It requires two text chunks, whereas triplet loss requires three text chunks. On the other hand, triplet loss often provides better results during training. Typically, the embedder is trained twice: initially with a contrastive loss function and later fine-tuned with a triplet loss function. \n",
    "\n",
    "An important point to mention is that the distance function in the triplet or contrastive loss can vary depending on the use case of the network. When trying to find semantic similarities, a cosine distance between the embeddings is used, whereas in different contexts, a norm distance like Euclidean distance can be used. \n",
    "\n",
    "Cosine distance is based on cosine similarity, or the cosine of the angle between the two embeddings. Cosine distance does not measure the size of the difference between the embeddings but how similar the direction is, as embeddings with close directions have values close to 0. Orthogonal embeddings have cosine values equal to 0, and embeddings that are oppositely directed have cosine values close to -1. The loss function needs to be a distance function, so just a cosine similarity between the embeddings won't be enough since it does not satisfy the distance requirements. That's why cosine distance is used:\n",
    "\n",
    "$$\n",
    "D(A, B) = 1 - \\cos(\\theta), \\text{ where } \\theta \\text{ is the angle between A and B}\n",
    "$$\n",
    "\n",
    "\n",
    "At this point, all of the components of the sentence embedding model are explained. A representative example of a sentence embedding model is SBERT, which is based on the popular LLM BERT. This resource will use SBERT as a Sentence Embedder Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Related work\n",
    "\n",
    "## General RAG systematization\n",
    "\n",
    "### Naive RAG \n",
    "\n",
    "Is the type of rag described in this document.\n",
    "\n",
    "### Advanced RAG \n",
    "\n",
    "Focuses on improving the retrieval of embeddings or data. It adds additional **pre-retrieval** and **post-retrieval** phase. \n",
    "In the pre-retrieve several optimization methods are employed like: \n",
    "* Query optimization\n",
    "* Storage optimization and retrieval\n",
    "* Extending the stored data with metadata\n",
    "In the **post-retrieval** phase idea is to create more usable context from the retrieved data. The data now is re-ranked and compressed, cleaned up [4]\n",
    "\n",
    "### Modular RAG \n",
    "\n",
    "This type of architecture tries to split the system in to more modules that can be independently scaled and optimized. \n",
    "Example: Query Processing Module, Retriever, Re-ranking, Context Management, Generation Module.\n",
    "\n",
    "\\newpage\n",
    "## Different RAG methods and models\n",
    "\n",
    "### RAG-Sequence Model\n",
    "\n",
    "This type of RAG retrieves $k$ embeddings, but unlike the standard RAG implementation it does not use all the embeddings at the same time to generate the answer. Before returning a token, the next token is generated $k$ times, meaning for every embedding one token is generated. Then the token with the highest probability is returned. This is done for every token.\n",
    "\n",
    "*The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized*\n",
    "\n",
    "### RAG-Token Model\n",
    "\n",
    "Interesting research has been done using the so-called *RAG-Token Model*. The RAG technique described, initially retrieves $k$ documents related to the question then augments the context of the question and sends the context and the question as an input to an LLM or the generator. The RAG-Token Model retrieves new documents on every generated token. Once a token is generated, the token is appended to all previously generated tokens and this are appended to the initial question. This results in a string of the following form \"Question + GenerateTockenList\". This new string is then used again as an input in the RAG, or as an input for the RAG retrieved. The process retrieves new documents for every new generated token and uses this documents to generate the next token. This makes sure that every new generated token is equally dependent on the question and the non-parametric memory.\n",
    "\n",
    "\n",
    "*RAG-Token Model, In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token* [3]\n",
    "\n",
    "## Iterative and Recursive RAG\n",
    "\n",
    "Iterative rag:\n",
    "One example is the \"Speculative RAG\" described in the paper [18].\n",
    "Based on the question it retrieves $N$ embeddings. The embeddings are then clustered in $k$ classes using K-means. From the $k$ clusters one embedding is selected per cluster, resulting in k embeddings. Then the algorithm uses two LLMs so called a drafter and a verifier. The drafter is used to generate the answer called a draft and explanation called rational. It is important to note that not all LLMs generate rationals. There are some that can do this, but usually the model needs to be trained to provide the rational. The RAG-Model from Hugginface provides explanations that can be used as a rational, also the retrieved embeddings might be used as rationals.\n",
    "Once the answer and the rational are generated, the quality of the answer and question is measured by the verifier.\n",
    "\n",
    "### Self-Consistency Score\n",
    "\n",
    "It gives the probability that the LLM can generate the answer and the rational based on the question. It is given by the joint probability formula:\n",
    "$$\n",
    "ρ_{sc} = P(A,B∣Q) = P(A|Q) * P(B|Q,A) \n",
    "$$\n",
    "Where:\n",
    "- A - answer \n",
    "- B - rational \n",
    "- Q - question \n",
    "\n",
    "The value can be retreated from the LLM. It simply says what is the probability that the answer been generated based on the question and the probability that the rational is generated based on the question and the answer.\n",
    "\n",
    "### Self-reflection score\n",
    "\n",
    "It tries to determine if the rational sports the answer. For this a new question R can be formed: Does the rational \"A\" sports the answer \"B\" for the question \"Q\"? The new question is called a self-reflected statement and denoted with R.\n",
    "Then the probability of Yes been generated from the LLM is measured.\n",
    "In general, the self-reflection score is the conditional probability of \"Yes\" been generated by the LLM given the input of a question Q, answer A, rational B and self reflected statement R.\n",
    "\n",
    "\n",
    "Self-reflection score:\n",
    "$$\n",
    "ρ_{sr} = P(\"Yes\"|Q, A, B, R)\n",
    "$$\n",
    "\n",
    "The process is repeated for all the clusters, and a $ρ_{sr}*ρ_{sc}$ is calculated. The answer with the highest $ρ_{sr}*ρ_{sc}$ score is picked as the most relevant, and it is the answer that is returned.\n",
    "\n",
    "## Recursive rag\n",
    "\n",
    "As any recursive algorithm it repeats its self until the results satisfies certain criteria. In case of the retrieval, a naive RAG system performs good when all the information required for answering the question is in the retrieved embedding, but it fails when this information is scattered in different embeddings. The Recursive rag starts examine the semantically similar documents not only of the question but also of the embeddings that are initially retrieved. With this it tries to capture all the relevant information. [16][17]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research methodology\n",
    "\n",
    "In this research, a simple RAG system will be coded. The system will be flexible and modular enough so can use different LLMS as generators. \n",
    "The system will also have a Retriever that will be able to create embeddings based on sentences and also on paragraphs. The retriever will also retrieve embeddings based on a question/query.\n",
    "All the chosen LLMs and their appropriate RAGs will be asked three different type of Open-domain question. This is a type of question that can't be answered with simple yes or no and requires accessing a broad range of information to provide an answer.\n",
    "\n",
    "The purpose of the experiments:\n",
    "1. Compare the differences between a RAG answer and a standard LLM. \n",
    "2. Assess the quality of the models for answering different type of questions\n",
    "\n",
    "\n",
    "### LLMs\n",
    "\n",
    "Depending on the LLMs type, three different LLM models will be used:\n",
    "\n",
    "1. Casual models \n",
    "\n",
    "2. Question answering models \n",
    "\n",
    "3. T5 - conversational models \n",
    "\n",
    "\n",
    "Regarding the model size, the experiments will be done on small LLMs ranging from 66M up to 1.3B.\n",
    "\n",
    "\n",
    "| Model                                           | Description                                  | Number of Parameters | Type                    |\n",
    "|-------------------------------------------------|----------------------------------------------|----------------------|-------------------------|\n",
    "| **blenderbot 90M**                              | Facebook AI's BlenderBot Model               | 90M                  | Causal Model            |\n",
    "| **GPT-2 124M**                                  | OpenAI's Generative Model                    | 124M                 | Causal Model            |\n",
    "| **GPT-Neo 125M**                                | EleutherAI's Generative Model                | 125M                 | Causal Model            |\n",
    "| **GPT-2 Medium 355M**                           | OpenAI's Generative Model                    | 355M                 | Causal Model            |\n",
    "| **GPT-2 Large 762M**                            | OpenAI's Generative Model                    | 762M                 | Causal Model            |\n",
    "| **GPT-Neo 1.3B**                                | EleutherAI's Generative Model                | 1.3B                 | Causal Model            |\n",
    "| **DistilBERT**                                  | Hugging Face's Optimized BERT for QA         | 66M                  | Question Answering Model |\n",
    "| **roberta-base**                                | Facebook AI's Optimized BERT for QA          | 125M                 | Question Answering Model |\n",
    "| **bert-large-uncased-whole-word-masking-finetuned-squad** | Google's NLU Model for QA  | 340M                 | Question Answering Model |\n",
    "| **T5 Base 220M**                                | Google’s Text-to-Text Transfer Transformer   | 220M                 | T5 - Conversational Model |\n",
    "| **T5 Large 770M**                               | Google’s Text-to-Text Transfer Transformer   | 770M                 | T5 - Conversational Model |\n",
    "\n",
    "\n",
    "\n",
    "*For comparison, the currently used production ChatGPT 3.5, according to ChatGPT, has a size of around 175B. ChatGPT 4.0 has not exposed any information about the number of parameters or architecture. Most probably, both models employ the RAG technique to improve their performance.*\n",
    "\n",
    "\n",
    "It is important to point out that the causal models are not trained for answering questions but for generating text based on input. Running a RAG around causal LLMs will give an idea of how powerful a RAG system can be. The question-answering models are more suitable for usage in a RAG system. They extract the continuous character set in the context that has the highest probability of being an answer given a specific question. The T5 models are conversational models, and although they are not specifically trained to answer questions, they are trained to maintain a conversation and should outperform the causal and question-answering LLMs.\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "The questions asked and their types are:\n",
    "1. Fact-based question - \"How tall is the Pyramid of Giza?\"\n",
    "2. List-based question - \"What materials were used in constructing the Great Wall of China?\"\n",
    "3. Synthesis-based Fact question - \"Which famous structures were either designed or structurally influenced by Gustave Eiffel?\"\n",
    "\n",
    "The \"Synthesis-based Fact\" question is a type of question where the answer needs to be derived from more than one retrieved fact. Unlike the other questions, where the answer is directly contained in the embedding, correctly answering the synthesis-based fact question implies that the retrieval system correctly mapped and retrieved the embeddings, and that the LLM is capable of drawing conclusions based on several facts.\n",
    "\n",
    "\n",
    "\\newpage\n",
    "### Retrieval and augmentation \n",
    "\n",
    "There will be 3 types of rag augmentation, in this text called retrieval: \n",
    "\n",
    "* Retrieval of one sentence, will create a context for the question from one sentence using a facts sentence based database.\n",
    "* Retrieval of one paragraph, will create a context for the question from one paragraph using a facts sentence based database.\n",
    "* Retrieval of three paragraphs, will create a context for the question from three paragraphs using a facts paragraph based database.\n",
    "The database for the sentences and paragraphs are created from two files containing semantically correct but not necessarily related sentences and paragraphs.\n",
    "\n",
    "\n",
    "### RAG solution\n",
    "The RAG system will be a manual solution, not an existing RAG system. The idea is to show that even a simple RAG system can extend the capabilities of the LLM.\n",
    "\n",
    "\n",
    "### Sentence Embedder\n",
    "For sentence embedder a SBART based embedder will be used `sentence_transformers import SentenceTransformer`\n",
    "\n",
    "#### Non-Parametric library\n",
    "For non-parametric library `faiss` will be used, it is more than enough to handle this experiment\n",
    "\n",
    "\n",
    "#### Environment \n",
    "Jupyter notebook\n",
    "\n",
    "\n",
    "#### Hardware  \n",
    "\n",
    "For the particular reasons an open sourced LLMs will be used that can be run locally on the pc. \n",
    "The chosen hardware is a m3 processor with 36GB of RAM.\n",
    "\n",
    "#### Experiments definition\n",
    "\n",
    "In total five experiments will be performed.\n",
    "\n",
    "| Experiment | Question Type            | Augmentation Type | Augmentations |\n",
    "|------------|--------------------------|-------------------|---------------|\n",
    "| 1          | Fact-based question       | Sentence          | 1             |\n",
    "| 2          | List-based question       | Sentence          | 1             |\n",
    "| 3          | Fact-based question       | Paragraph         | 1             |\n",
    "| 4          | List-based question       | Paragraph         | 1             |\n",
    "| 5          | Synthesis-based Fact question | Paragraph     | 3             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and discussion\n",
    "\n",
    "### Experiment 1: Fact-based Question - \"How tall is the Pyramid of Giza?\"\n",
    "\n",
    "\n",
    "#### Experiment Description\n",
    "\n",
    "In this experiment, various language models were tested to answer the fact-based question: \"How tall is the Pyramid of Giza?\". \n",
    "The experiment parameters are as follows:\n",
    "\n",
    "- **Question Type**: Fact-based\n",
    "- **Question**: \"How tall is the Pyramid of Giza?\"\n",
    "- **Retrieval Type**: Sentence retrieval\n",
    "- **Number of Retrievals**: 1\n",
    "\n",
    "\n",
    "### Causal Models\n",
    "\n",
    "| LLM                     | LLM Answer                                                                                                                | RAG Answer                                                                                                 |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|\n",
    "| facebook/blenderbot-90M  | balkicker mutations mutations pment mutations mutations mutations kicker fiers eston eston eston eres bender eston ...    | lease willis lease mative mative lease willis mative lease lease lease willis ...                          |\n",
    "| gpt2                    | The Pyramid of Giza is the tallest building in Egypt. The Pyramid of Giza is located in the Giza Plateau ...              | The Great Pyramid of Giza is the largest and oldest of the three pyramids, standing at 146.6 meters ...     |\n",
    "| EleutherAI/gpt-neo-125M  | The Pyramid of Giza is the tallest pyramid in the world. It was completed in the 13th century AD ...                      | The Pyramid of Giza is the largest and oldest of the three pyramids, standing at 146.6 meters ...           |\n",
    "| gpt2-medium              | The Pyramid of Giza is the tallest structure in Egypt. It is 1,068 feet tall, and is one of the largest pyramids ...       | The Pyramid of Giza is the largest and oldest of the three pyramids, standing at 146.6 meters ...           |\n",
    "| gpt2-large               | The Pyramid of Giza is the tallest structure in the world. How tall is the Taj Mahal? ...                                 | The Great Pyramid of Giza is the largest and oldest of the three pyramids, standing at 146.6 meters ...     |\n",
    "| EleutherAI/gpt-neo-1.3B  | The Pyramid of Giza is one of the Seven Wonders of the Ancient World. It is one of the Seven Wonders of the Modern World ... | The pyramid of Giza, also known as the Pyramid of Khufu, is the largest and oldest of the three pyramids ... |\n",
    "\n",
    "In all the cases the casual models generated a wrong LLM answer. Except the \"facebook/blenderbot-90M\" they all generated grammatically correct sentences. Concerning the RAG answers, all of them generated a correct RAG answer except the \"facebook/blenderbot-90M\". This is an amazing result since without any additional training or tuning, the models when using a RAG approach were able to generate a correct answer. Taking in to consideration the parameters the smallest model that give a correct answer was GPT2 with 90M.\n",
    "\n",
    "### QA Models\n",
    "\n",
    "| LLM                                                | LLM Answer                                             | RAG Answer                     |\n",
    "|----------------------------------------------------|--------------------------------------------------------|--------------------------------|\n",
    "| distilbert-base-uncased-distilled-squad_66M            | how tall is the pyramid of giza? [SEP]                 | 146.6 meters                   |\n",
    "| deepset/roberta-base-squad2_125M                        | <s>                                                    | 146.6 meters (481 feet)        |\n",
    "| bert-large-uncased-whole-word-masking-finetuned-squad_340M | tall                                                  | 146.6 meters                   |\n",
    "\n",
    "The QA models are also answered correctly. Compared to the casual LLMs RAG generated answer, here the models perform better as a model of 66M was able to answer the question correctly. The generated LLM answer was wrong for every model.\n",
    "\n",
    "### T5 Models\n",
    "\n",
    "| LLM         | LLM Answer | RAG Answer               |\n",
    "|-------------|------------|--------------------------|\n",
    "| t5-base_220M     | None       | 146.6 meters (481 feet)   |\n",
    "| t5-large_770M    | False      | 140 meters (481 feet)     |\n",
    "\n",
    "The T5 models RAG generated answer is also correct for all the models. At this point they perform the quality of the answer is on the same level of the QA models and Casual models. But the models a bigger than the QA models.\n",
    "\n",
    "Conclusion: Given the size of the models the QA models are most appropriate for answering Fact-based Question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: List-based Question - \"What materials were used in constructing the Great Wall of China?\"\n",
    "\n",
    "#### Experiment Description\n",
    "\n",
    "In this experiment, various language models were tested to answer the list-based question: \"What materials were used in constructing the Great Wall of China?\". The experiment parameters are as follows:\n",
    "\n",
    "- **Question Type**: List-based Question\n",
    "- **Question**: \"What materials were used in constructing the Great Wall of China?\"\n",
    "- **Retrieval Type**: Sentence retrieval\n",
    "- **Number of Retrievals**: 1\n",
    "\n",
    "### Causal Models\n",
    "\n",
    "| LLM                     | LLM Answer                                                                                                                | RAG Answer                                                                                                 |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|\n",
    "| facebook/blenderbot-90M  | pos sitter sitter drip ba ba ba le diving ba ba bee ba ba da ba ba cats ba ba table da ba da disc ba ba orba da da prba ba prda da __end__ | scibal pubalfatty composition asses sciscifatty fatty bee pumubee bee mubee zarbalpuscibal mutbal bee bal composition fatty composition fatty bee bal strip scimutcomposition __end__ |\n",
    "| gpt2                    | The Great Wall of China is the largest in the world, and is the largest city in the world. It is the largest city in the world. It is the largest city in | The Great Wall of China is made of various materials, including stone, brick, tamped earth, and wood. The Great Wall of China was built by the People's Republic of ... |\n",
    "| EleutherAI/gpt-neo-125M  | The Great Wall of China was constructed by the Chinese government in the 17th century, and is known as the Great Wall of China ... | The Great Wall of China is made of various materials, including stone, brick, tamped earth, and wood. Answer What materials were used in constructing the Great Wall of China? |\n",
    "| gpt2-medium              | The Great Wall of China was built by the Ming Dynasty (1644-1911). The Great Wall was constructed of a combination of stone, wood, and metal ... | 'Answer The Great Wall of China was built of various materials, including stone, brick, tamped earth, and wood. The Great Wall of China was constructed of various materials, including stone ... |\n",
    "| gpt2-large               | The Great Wall of China was built by the Qin Dynasty (221-206 B.C.), which lasted from 221 to 206 B.C. ... | 'The Great Wall of China is made of various materials, including stone, brick, tamped earth, and wood.'Answer What materials were used in constructing the Great Wall of China? |\n",
    "| EleutherAI/gpt-neo-1.3B  | The Great Wall of China was constructed using many different types of materials, including wood, stone, brick, clay, and iron ... | The Great Wall of China is made of various materials, including stone, brick, tamped earth, and wood. |\n",
    "\n",
    "In all cases, the causal models generated incorrect or irrelevant LLM answers, often straying off-topic or providing erroneous information. The RAG approach, however, performed significantly better, with all models providing correct information about the materials used in constructing the Great Wall of China. This demonstrates the effectiveness of RAG in generating accurate answers even when the base model's output is flawed.\n",
    "\n",
    "### QA Models\n",
    "\n",
    "| LLM                                                | LLM Answer                                             | RAG Answer                     |\n",
    "|----------------------------------------------------|--------------------------------------------------------|--------------------------------|\n",
    "| distilbert-base-uncased-distilled-squad_66M            | what materials were used in constructing the great wall of china? [SEP]                 | stone, brick, tamped earth, and wood                   |\n",
    "| deepset/roberta-base-squad2_125M                        | <s>                                                    | stone, brick, tamped earth, and wood        |\n",
    "| bert-large-uncased-whole-word-masking-finetuned-squad_340M |                                                      | stone, brick, tamped earth, and wood                   |\n",
    "\n",
    "The QA models performed well in generating correct RAG answers, accurately listing the materials used in constructing the Great Wall of China. Similar to the first experiment, the generated LLM answers were incorrect, but the RAG system compensated for this by providing correct and relevant answers, showing its strength in retrieving and presenting accurate information.\n",
    "\n",
    "### T5 Models\n",
    "\n",
    "| LLM                                                | LLM Answer                                             | RAG Answer                     |\n",
    "|----------------------------------------------------|--------------------------------------------------------|--------------------------------|\n",
    "| t5-base_220M                                         | None                                                    | stone, brick, tamped earth, and wood                     |\n",
    "| t5-large_770M                                        | False                                                   | stone, brick, tamped earth, and wood                     |\n",
    "\n",
    "The T5 models also demonstrated strong performance in the RAG approach, correctly listing the materials used in constructing the Great Wall of China. However, like the other models, their direct LLM answers were inaccurate or non-informative, further highlighting the importance of RAG in ensuring accurate output.\n",
    "\n",
    "\n",
    "Conclusion: Given the size of the models the QA models are most appropriate for answering List-based Question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Fact-based Question - \"How tall is the Pyramid of Giza?\"\n",
    "\n",
    "#### Experiment Description\n",
    "\n",
    "In this experiment, various language models were tested to answer the fact-based question: \"How tall is the Pyramid of Giza?\" using paragraph retrieval with one retrieved document. Storing paragraphs is more efficient than storing sentences, as the size of the database will be smaller and paragraphs hold more context and information than the sentences. The experiment parameters are as follows:\n",
    "\n",
    "- **Question Type**: Fact-based Question\n",
    "- **Question**: \"How tall is the Pyramid of Giza?\"\n",
    "- **Retrieval Type**: Paragraph retrieval\n",
    "- **Number of Retrievals**: 1\n",
    "\n",
    "### Causal Models\n",
    "\n",
    "| LLM                     | LLM Answer                                                                                                                | RAG Answer                                                                                                 |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|\n",
    "| facebook/blenderbot-90M  | pickbalfiers fiers predators mutations bender mutations mutations eston mutations eston predators kicker mutations mutations predators mutations pment eston eston eston trout mutations fiers eston eston bender eston eston kicker eston eston fiers eston joreston __end__ | design mutpuwillis willis design lease orpotbee willis potpotdistribution fatty fatty composition lease mutfatty fatty willis strip spy lease willis distribution willis strip distribution mutdraft mutpupumutspy __end__ |\n",
    "| gpt2                    | The Pyramid of Giza is the tallest building in the world. It is located in the northern part of the country of Egypt. It is one of the tallest buildings in the world | The Great Pyramid of Giza is the tallest man-made structure in the world for over 3,800 years until the construction of Lincoln Cathedral in England in 1311. The Sphinx is |\n",
    "| EleutherAI/gpt-neo-125M  | How tall is the Pyramid of Giza? How tall is the Pyramid of Giza? How tall is the Pyramid of Giza? How tall is | The Great Pyramid of Giza was the tallest man-made structure in the world for over 3,800 years until the construction of Lincoln Cathedral in England in 1311. The Great |\n",
    "| gpt2-medium              | The Pyramid of Giza, also known as the Great Pyramid of Giza or Pyramid of the Sun, is one of the world's largest pyramids. It was built in Egypt | The Pyramid of Giza is the tallest man-made structure in the world for over 3,800 years until the construction of Lincoln Cathedral in England in 1311. The Sphinx, |\n",
    "| gpt2-large               | The Pyramid of Giza is believed to be the tallest structure in the world. The Pyramid of Giza is located in Egypt. The Pyramid of Giza was built between | The height of the Great Pyramid of Giza is 146.6 meters (481 feet). This is the tallest man-made structure in the world for over 3,800 years. The |\n",
    "| EleutherAI/gpt-neo-1.3B  | This is a question that has been asked many times, but the answer is not always so simple. Some of the tallest buildings in the world are located in the United States, and | The Pyramids of Giza are the only remaining structure of the Seven Wonders of the Ancient World. The Great Pyramid of Giza, also known as the Pyramid of Khuf |\n",
    "\n",
    "In this experiment, the causal models generally provided incorrect or nonsensical LLM answers. However, the RAG approach performed significantly better, producing correct answers by accurately retrieving and presenting the relevant information from the paragraph.\n",
    "\n",
    "### QA Models\n",
    "\n",
    "| LLM                                                | LLM Answer                                             | RAG Answer                     |\n",
    "|----------------------------------------------------|--------------------------------------------------------|--------------------------------|\n",
    "| distilbert-base-uncased-distilled-squad_66M            | how tall is the pyramid of giza? [SEP]                 | 146.6 meters                   |\n",
    "| deepset/roberta-base-squad2_125M                        | <s>                                                    | 146.6 meters (481 feet)        |\n",
    "| bert-large-uncased-whole-word-masking-finetuned-squad_340M | tall                                                  | 146.6 meters                   |\n",
    "\n",
    "The QA models continued to perform well, with the RAG approach providing correct and relevant answers. Despite the direct LLM answers being incorrect, the RAG system ensured accurate information was retrieved and presented.\n",
    "\n",
    "### T5 Models\n",
    "\n",
    "| LLM                                                | LLM Answer                                             | RAG Answer                     |\n",
    "|----------------------------------------------------|--------------------------------------------------------|--------------------------------|\n",
    "| t5-base_220M                                         | None                                                    | 146.6 meters (481 feet)                     |\n",
    "| t5-large_770M                                        | False                                                   | 146.6 meters                     |\n",
    "\n",
    "The T5 models also demonstrated strong performance in the RAG approach, consistently providing the correct height of the Pyramid of Giza. As seen in previous experiments, the direct LLM answers were inaccurate, underscoring the importance of using RAG for accurate outputs.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    " The QA models and T5 models were particularly efficient, consistently providing the correct height of the Pyramid of Giza. Given the size of the models, the most efficient for answering Fact-based Question with one paragraph retrieval are the QA models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: List-based Question - \"What materials were used in constructing the Great Wall of China?\"\n",
    "\n",
    "#### Experiment Description\n",
    "\n",
    "In this experiment, various language models were tested to answer the list-based question: \"What materials were used in constructing the Great Wall of China?\" using paragraph retrieval with one retrieved document. The experiment parameters are as follows:\n",
    "\n",
    "- **Question Type**: List-based Question\n",
    "- **Question**: \"What materials were used in constructing the Great Wall of China?\"\n",
    "- **Retrieval Type**: Paragraph retrieval\n",
    "- **Number of Retrievals**: 1\n",
    "\n",
    "### Causal Models\n",
    "\n",
    "| LLM                     | LLM Answer                                                                                                                | RAG Answer                                                                                                 |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|\n",
    "| facebook/blenderbot-90M  | horn ba ba squba ba ba da ba ba bal ba ba phba ba eling ba ba sheet ba ba itba da phth ba ba muba da da audprba da __end__ | potpotscistrip scipotorscizarpotdistribution scipotstrip strip cupotstrip sciscistrip potsciscibee qpotstrip cuqstrip potqsciqstrip sci__end__ |\n",
    "| gpt2                    | There are many materials that were used in the construction of the Great Wall of China, but only a few were used for the construction of the Great Wall of China ... | The Great Wall of China was built to protect against invasions from northern tribes. The Great Wall of China is made of various materials, including stone, brick, tamped earth ... |\n",
    "| EleutherAI/gpt-neo-125M  | The Great Wall of China was built by the Chinese government during the reign of the Qing dynasty ... | The Great Wall of China is over 13,000 miles long.  Construction of the Great Wall of China began in the 7th century BC. The Great Wall of China was ... |\n",
    "| gpt2-medium              | The Great Wall of China was constructed by a team of Chinese engineers, engineers, and architects ... | The Great Wall of China was constructed of various materials, including stone, brick, tamped earth, and wood. The height of the Great Wall of China varies, with the tallest ... |\n",
    "| gpt2-large               | The Great Wall of China was constructed by the Han Dynasty (206 BC – 220 AD) ... | The Great Wall of China is made of various materials, including stone, brick, tamped earth, and wood. The height of the Great Wall of China varies, with the tallest sections ... |\n",
    "| EleutherAI/gpt-neo-1.3B  | The Great Wall of China is one of the most famous examples of ancient Chinese architecture ... | The Great Wall of China is made of various materials, including stone, brick, tamped earth, and wood The height of the Great Wall of China varies, with the tallest ... |\n",
    "\n",
    "In this experiment, the causal models generally provided incorrect or nonsensical LLM answers. The RAG approach, however, showed improvement, with more accurate information being retrieved and presented. Despite this, the answers still lacked consistency in listing all the materials used in the construction of the Great Wall of China.\n",
    "\n",
    "### QA Models\n",
    "\n",
    "| LLM                                                | LLM Answer                                             | RAG Answer                     |\n",
    "|----------------------------------------------------|--------------------------------------------------------|--------------------------------|\n",
    "| distilbert-base-uncased-distilled-squad_66M            | what materials were used in constructing the great wall of china? [SEP]                 | stone, brick, tamped earth, and wood                   |\n",
    "| deepset/roberta-base-squad2_125M                        | <s>                                                    | stone, brick, tamped earth, and wood        |\n",
    "| bert-large-uncased-whole-word-masking-finetuned-squad_340M |                                                      | stone, brick, tamped earth, and wood                   |\n",
    "\n",
    "The QA models performed exceptionally well in generating correct RAG answers, accurately listing the materials used in constructing the Great Wall of China. The direct LLM answers were not particularly useful, but the RAG system effectively retrieved and presented the necessary information.\n",
    "\n",
    "### T5 Models\n",
    "\n",
    "| LLM                                                | LLM Answer                                             | RAG Answer                     |\n",
    "|----------------------------------------------------|--------------------------------------------------------|--------------------------------|\n",
    "| t5-base_220M                                         | None                                                    | stone, brick, tamped earth, and wood                     |\n",
    "| t5-large_770M                                        | False                                                   | stone, brick, tamped earth, and wood                     |\n",
    "\n",
    "The T5 models also performed well in the RAG approach, consistently providing the correct list of materials used in constructing the Great Wall of China. Similar to other models, the direct LLM answers were inaccurate or non-informative, but the RAG system ensured accurate output.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The QA models proved to be the most efficient for answering list-based questions with paragraph retrievals. Their performance in generating accurate RAG answers highlights their suitability for RAG-based solutions, especially when detailed and precise information is required. Given the size and efficiency, the QA models are the best choice for implementing a RAG solution in this context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: Synthesis-based Question - \"Which famous structures, both designed or structurally influenced by Gustave Eiffel?\"\n",
    "\n",
    "#### Experiment Description\n",
    "\n",
    "In this experiment, various language models were tested to answer the synthesis-based question: \"Which famous structures, both designed or structurally influenced by Gustave Eiffel?\" using paragraph retrieval with three retrieved documents. The experiment parameters are as follows:\n",
    "\n",
    "- **Question Type**: Synthesis-based Question\n",
    "- **Question**: \"Which famous structures, both designed or structurally influenced by Gustave Eiffel?\"\n",
    "- **Retrieval Type**: Paragraph retrieval\n",
    "- **Number of Retrievals**: 3\n",
    "\n",
    "The synthesis-based question require that several facts are take in to consideration inorder to answer the quiestion. In this case the retreaver will need provide the generator several paragraphs with information on Gustave Eiffel. Then the LLM needs combine the facts in to conclusion and generate the answer. \n",
    "\n",
    "### Causal Models\n",
    "\n",
    "| LLM                     | LLM Answer                                                                                                                | RAG Answer                                                                                                 |\n",
    "|-------------------------|---------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|\n",
    "| facebook/blenderbot-90M  | etically ication ix belle ication belle ication judgment ication belle le tacication belle conception ication le le belle ication planted le tacdden ication tacication judge ix ication le ication judge belle ication ication le __end__ | Error generating text for facebook/blenderbot-90M |\n",
    "| gpt2                    | They're not, but it's hard not to feel a twinge of nostalgia for them ... | The Eiffel Tower is located in Paris, France. The Eiffel Tower was completed in 1889. The Eiffel Tower was painted every seven years to prevent it ... |\n",
    "| EleutherAI/gpt-neo-125M  | This article is part of a series of articles that explore Gustave Eiffel’s influence on architecture and design ... | The Colosseum was completed in AD 80. The Colosseum was designed by Gustave Eiffel, who also designed the Eiffel Tower. |\n",
    "| gpt2-medium              | There are many, many, many. Some of the most famous of them are: The Eiffel Tower, Paris ... | The Eiffel Tower is located in Paris, France. The Eiffel Tower was completed in 1889. The Eiffel Tower is 324 meters tall. The Eiffel Tower was designed by Gustave Eiffel. |\n",
    "| gpt2-large               | The Louvre, Paris, France The Eiffel Tower, Paris, France The Eiffel Tower, Paris, France ... | The Eiffel Tower is located in Paris, France. The Eiffel Tower is 324 meters tall. The Eiffel Tower was designed by Gustave Eiffel. |\n",
    "| EleutherAI/gpt-neo-1.3B  | The Eiffel Tower in Paris. The Eiffel Tower in Paris. The Eiffel Tower in Paris ... | The Eiffel Tower is located in Paris, France. The Eiffel Tower was completed in 1889. The Eiffel Tower is 324 meters tall. |\n",
    "\n",
    "In this experiment, the causal models struggled to produce accurate or relevant LLM answers. The RAG approach helped improve the accuracy but was still limited, with most models failing to provide a comprehensive list of structures designed or influenced by Gustave Eiffel. The answers often lacked synthesis, indicating that causal models might not be the best choice for complex synthesis-based questions.\n",
    "\n",
    "### QA Models\n",
    "\n",
    "| LLM                                                | LLM Answer                                             | RAG Answer                     |\n",
    "|----------------------------------------------------|--------------------------------------------------------|--------------------------------|\n",
    "| distilbert-base-uncased-distilled-squad_66M            | which famous structures, both designed or structurally influenced by gustave eiffel? [SEP]                 | statue of liberty                   |\n",
    "| deepset/roberta-base-squad2_125M                        | <s>                                                    | <s>        |\n",
    "| bert-large-uncased-whole-word-masking-finetuned-squad_340M |                                                      | statue of liberty                   |\n",
    "\n",
    "The QA models had mixed performance in this experiment. While the direct LLM answers were often uninformative, the RAG answers did manage to identify at least one structure associated with Gustave Eiffel, but they lacked completeness. The synthesis required for this question appeared challenging for these models.\n",
    "\n",
    "### T5 Models\n",
    "\n",
    "| LLM                                                | LLM Answer                                             | RAG Answer                     |\n",
    "|----------------------------------------------------|--------------------------------------------------------|--------------------------------|\n",
    "| t5-base_220M                                         | None                                                    | The Eiffel Tower                     |\n",
    "| t5-large_770M                                        | False                                                   | Eiffel Tower and The Statue of Liberty                     |\n",
    "\n",
    "The T5 models performed relatively well in the RAG approach, with the larger model (T5-large) being able to correctly list both the Eiffel Tower and the Statue of Liberty as structures associated with Gustave Eiffel. This highlights the potential of T5 models in synthesis-based tasks, especially when more complex reasoning is required.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "For synthesis-based questions, the T5 models, particularly the larger variant, demonstrated the most potential in the RAG approach, successfully identifying multiple structures associated with Gustave Eiffel. Although the QA models could partially answer the question, they struggled with the complexity of synthesizing information from multiple documents. The causal models, even with RAG, were not able to handle this task effectively. Therefore, T5 models are the best choice for RAG-based solutions when dealing with synthesis-based questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "This text demonstrates through experimentation that Retrieval-Augmented Generation (RAG) systems effectively address some of the most significant challenges faced by Large Language Models (LLMs) and other models with parametric memory. Compared to models solely relying on parametric memory, RAG systems offer several advantages:\n",
    "\n",
    "- **Horizontally Scalable**: RAG systems can efficiently scale by distributing the retrieval process across multiple nodes, allowing for handling larger datasets and more complex queries.\n",
    "- **Distributive**: The modular nature of RAG systems enables distribution of tasks across different components, enhancing robustness and flexibility.\n",
    "- **Mitigates High Bias**: By integrating external knowledge retrieval, RAG systems reduce the need for extremely large models with numerous parameters, thus avoiding issues related to under-fitting and high bias in therms of the real world data set, or the cross validation set The model is just not curved enough to handle real world data.\n",
    "- **Performance Enhancement**: RAG systems can surpass the performance of their base generative models by leveraging external knowledge, resulting in more accurate and contextually relevant responses.\n",
    "\n",
    "As an example, a decent quality question answering system can be created using only a 66M parameter pertained question answering model like DistilBERT when using an index like FAISS. However, it is crucial that the paragraphs and sentences provided to the model contain the answer explicitly written, with no pronouns or ambiguous references. In case of the T5 models they will perform better than QA in a more complex text scenarios, these models are capable of understanding and synthesizing information. The t5-large_770M given the size and performance would be a good candidate more general RAG solution. \n",
    "\n",
    "\n",
    "In summary, RAG systems provide a scalable, distributive, and cost-efficient solution to enhance the capabilities of LLMs, addressing key limitations and improving overall performance without the necessity for excessively large and complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Attention Is All You Need, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin, [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)\n",
    "2. The Faiss Library, Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou, [https://arxiv.org/pdf/2401.08281](https://arxiv.org/pdf/2401.08281)\n",
    "3. Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, [https://arxiv.org/pdf/2005.11401v4](https://arxiv.org/pdf/2005.11401v4)\n",
    "4. Retrieval-Augmented Generation for Large Language Models: A Survey, Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang, [https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997)\n",
    "5. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, Nils Reimers, Iryna Gurevych, [https://arxiv.org/pdf/1908.10084](https://arxiv.org/pdf/1908.10084)\n",
    "6. Retrieval-augmented Generation (RAG) Explained: Understanding Key Concepts, Phil Miesle, [https://www.datastax.com/guides/what-is-retrieval-augmented-generation](https://www.datastax.com/guides/what-is-retrieval-augmented-generation)\n",
    "7. The Comprehensive Guide to Retrieval Augmented Generation LLMs: Transforming Data into Knowledge, Dorota Owczarek, [https://nexocode.com/blog/posts/retrieval-augmented-generation-rag-llms](https://nexocode.com/blog/posts/retrieval-augmented-generation-rag-llms)\n",
    "8. Building high-quality RAG systems, Jesper Alkestrup, Nikolaj Skov Purup, [https://implementconsultinggroup.com/article/building-high-quality-rag-systems](https://implementconsultinggroup.com/article/building-high-quality-rag-systems)\n",
    "9. Optimizing RAG: A Guide to Choosing the Right Vector Database, Mutahar Ali, Medium, [https://medium.com/@mutahar789/optimizing-rag-a-guide-to-choosing-the-right-vector-database-480f71a33139](https://medium.com/@mutahar789/optimizing-rag-a-guide-to-choosing-the-right-vector-database-480f71a33139)\n",
    "10. Best Practices: Insert Vectors, Cloudflare Docs, [https://developers.cloudflare.com/vectorize/best-practices/insert-vectors/](https://developers.cloudflare.com/vectorize/best-practices/insert-vectors/)\n",
    "11. Best Vector DBs for Retrieval Augmented Generation (RAG), Kristen Kehrer, Aporia, [https://www.aporia.com/learn/best-vector-dbs-for-retrieval-augmented-generation-rag/](https://www.aporia.com/learn/best-vector-dbs-for-retrieval-augmented-generation-rag/)\n",
    "12. Retrieval-Augmented Generation (RAG), Databricks, [https://www.databricks.com/glossary/retrieval-augmented-generation-rag](https://www.databricks.com/glossary/retrieval-augmented-generation-rag)\n",
    "13. Atlas CLI: Deploy Local, MongoDB Docs, [https://www.mongodb.com/docs/atlas/cli/current/atlas-cli-deploy-local/](https://www.mongodb.com/docs/atlas/cli/current/atlas-cli-deploy-local/)\n",
    "14. RAG with Atlas Vector Search and MongoDB, Vincent Benedict M.W, Medium, [https://medium.com/@if-21062/rag-with-atlas-vector-search-mongodb-0d2420e00b64](https://medium.com/@if-21062/rag-with-atlas-vector-search-mongodb-0d2420e00b64)\n",
    "15. Neo4j Langchain Vector Index Implementation, Tomaž Bratanič, Graph ML and GenAI Research, Neo4j, [https://neo4j.com/developer-blog/neo4j-langchain-vector-index-implementation/](https://neo4j.com/developer-blog/neo4j-langchain-vector-index-implementation/)\n",
    "16. Advanced RAG: Recursive Retrieval with llamaindex, [https://www.pondhouse-data.com/blog/advanced-rag-recursive-retrieval-with-llamaindex]\n",
    "17. Dense Passage Retrieval for Open-Domain Question Answering, Vladimir Karpukhin, Barlas Oguz, Sewon Min†, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih, [https://arxiv.org/pdf/2004.04906](https://arxiv.org/pdf/2004.04906)\n",
    "18. Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting, Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, Chen-Yu Lee, Tomas Pfister [https://arxiv.org/pdf/2407.08223]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix: Rag Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/marko_grabuloski/Documents/fak/RagSystems/.conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - numpy\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2024.7.4-~ --> pkgs/main/osx-arm64::certifi-2024.7.4-py311hca03da5_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/marko_grabuloski/Documents/fak/RagSystems/.conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - detectron2\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-arm64::certifi-2024.7.4~ --> conda-forge/noarch::certifi-2024.7.4-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Prepare required libraries for \n",
    "%conda install -y numpy\n",
    "# Retrieval and storage required libraries\n",
    "%conda install -y sentence-transformers\n",
    "%conda install -y faiss-cpu\n",
    "# LLMs related libraries\n",
    "%conda install -y transformers\n",
    "%conda install -y pytorch\n",
    "%conda install -y ipywidgets\n",
    "%conda install -y protobuf\n",
    "%conda install -c conda-forge detectron2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# smoke test\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LLM related libs\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Retrieval and Storage Related libs\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def read_and_split_sentences(file_path)->list[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    sentences = re.split(r'(?<!\\d)\\.\\s+', text.strip())\n",
    "    return sentences\n",
    "\n",
    "def read_and_split_paragraphs(file_path) -> list[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    paragraphs = text.strip().split('\\n\\n')\n",
    "    return paragraphs\n",
    "\n",
    "def save_dict_to_json_file(data, file_name):\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self, model_name_in='all-MiniLM-L6-v2'):\n",
    "        self.documents = []\n",
    "        self.model = SentenceTransformer(model_name_in)\n",
    "        self.document_embeddings = None\n",
    "        # dimension is 384 for the 'all-MiniLM-L6-v2'\n",
    "        d = self.model.get_sentence_embedding_dimension()\n",
    "        self.index = faiss.IndexFlatL2(d)\n",
    "        \n",
    "    def add_documents(self, new_documents_in: list[str]):\n",
    "        # MaximumCharacters ≈ 512tokens×4characters/token=2048characters\n",
    "        new_document_embeddings = self.model.encode(new_documents_in).astype(np.float32)\n",
    "        self.index.add(new_document_embeddings)\n",
    "        self.documents.extend(new_documents_in)\n",
    "\n",
    "    def retrieve_documents(self, query_in, results_size_in=1):\n",
    "        query_embedding = self.model.encode([query_in]).astype(np.float32)\n",
    "        distances, indices = self.index.search(query_embedding, results_size_in)\n",
    "        retrieved_documents = [self.documents[idx] for idx in indices[0]]\n",
    "        return retrieved_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- \n",
    "# Terms\n",
    "* Generative AI -Generative AI refers to artificial intelligence technologies that create new content, ranging from text and images to code, by learning patterns from existing data. `https://www.datastax.com/guides/what-is-generative-ai`\n",
    "\n",
    "\n",
    "## Papers\n",
    "* https://arxiv.org/abs/2005.11401v4 facebook on rag initial document *\n",
    "\n",
    "\n",
    "## Links\n",
    "* https://www.datastax.com/guides/what-is-retrieval-augmented-generation\n",
    "* https://ar5iv.labs.arxiv.org/html/2312.10997 - academic\n",
    "* https://nexocode.com/blog/posts/retrieval-augmented-generation-rag-llms/\n",
    "* [https://www.datastax.com/guides/what-is-retrieval-augmented-generation](https://www.datastax.com/guides/what-is-retrieval-augmented-generation)\n",
    "* https://ar5iv.labs.arxiv.org/html/2404.07220v1 - academic\n",
    "* https://implementconsultinggroup.com/article/building-high-quality-rag-systems\n",
    "\n",
    "<!-- https://www.langchain.com/ -->\n",
    "<!-- \n",
    "Todo \n",
    "* Denced vectors - A dense vector index is a data structure that stores these dense vectors for efficient retrieval and similarity search. Here’s how it works in the context of Wikipedia:\n",
    "\n",
    "* We compare two RAG formulations, one which conditions on the same retrieved passages\n",
    "across the whole generated sequence, and another which can use different passages\n",
    "per token\n",
    "\n",
    "* tip na prasana\n",
    "    * open-domain question answering [5, 29],\n",
    "    * fact checking [56], \n",
    "    * fact completion [48]\n",
    "    * long-form question answering [12] -->\n",
    "  <!-- * Wikipedia articlegeneration [36], \n",
    "    * dialogue [41, 65, 9, 13], \n",
    "    * translation [17] \n",
    "    * language modeling [19, 27]\n",
    "\n",
    "* bias varianse -->\n",
    "\n",
    "<!-- \n",
    "Rag system:\n",
    "* We introduce RAG models where the parametric\n",
    "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
    "vector index of Wikipedia\n",
    "\n",
    "* useful links reaserch papers\n",
    "1. https://arxiv.org/pdf/2005.11401v4 rag experiment\n",
    "2. https://arxiv.org/pdf/2312.10997 rag clasification\n",
    "3. https://ar5iv.labs.arxiv.org/html/2404.07220v1 - zboruva za 'Blended Retrievers' ili ’Blended RAG' Dense Vector indexes and Sparse Encoder indexes, definirajgi poimite i tuka ima tabela za retreavcel performance na tocnost\n",
    "4. https://arxiv.org/abs/2401.08281 fass library\n",
    "\n",
    "LLM Transformer\n",
    "Sentence -> Tokenization -> Embedded Vectors -> Contextual Vectors\n",
    "\n",
    "Sentence Embedding Models\n",
    "Sentence -> Tokenization -> Embedded Vectors -> Contextual Vectors -> (Mean Pooling,  Max Pooling, CLS Token, Attention-based Pooling)\n",
    " -> Sentence Vector\n",
    "Mean Pooling calculates the average vector for every sentence -->\n",
    "<!-- \n",
    "* SBERT: Utilizes siamese and triplet network structures for efficient and meaningful sentence embeddings.\n",
    "* Dimensionality: Optimal dimensionality is crucial for retaining information in embeddings; strategies to minimize performance loss involve focusing on both encoder and pooler components.\n",
    "* Pooling Methods: Various pooling methods (CLS token, mean pooling, max pooling) affect the quality of sentence embeddings differently.\n",
    "* In-Context Learning: Large language models benefit from in-context learning to generate better sentence embeddings.\n",
    "\n",
    "1. Sentence-BERT (SBERT)\n",
    "The paper \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\" by Reimers and Gurevych (2019) introduces SBERT, which modifies the BERT network to create semantically meaningful sentence embeddings. SBERT employs a siamese network structure, allowing it to compute sentence embeddings that can be efficiently compared using cosine similarity. This structure enables SBERT to outperform traditional BERT in tasks like Semantic Textual Similarity (STS) by using mean pooling over token embeddings and fine-tuning on sentence pairs with a triplet loss objective​ (ar5iv)​. https://ar5iv.labs.arxiv.org/html/1908.10084\n",
    "\n",
    "2. Dimensionality of Sentence Embeddings\n",
    "The paper \"On the Dimensionality of Sentence Embeddings\" provides an empirical analysis of how the dimensionality of sentence embeddings affects performance across various tasks. It demonstrates that reducing the dimensionality of embeddings can lead to performance loss, not just due to the reduced dimension but also due to a decrease in the quality of the encoder's output. This study offers strategies for mitigating performance loss by separately addressing the encoder and pooler components​ (ar5iv)​.https://ar5iv.labs.arxiv.org/html/2310.15285\n",
    "\n",
    "3. Evaluation of BERT and ALBERT Sentence Embeddings\n",
    "In \"Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks,\" the authors evaluate the effectiveness of BERT and ALBERT models in generating sentence embeddings for tasks like Semantic Textual Similarity (STS) and Natural Language Inference (NLI). The study compares different pooling strategies (CLS token, average pooling, max pooling) and highlights the effectiveness of SBERT and SALBERT, which use siamese and triplet network structures for enhanced performance​ (ar5iv)​. https://ar5iv.labs.arxiv.org/html/2101.10642v1\n",
    "\n",
    "4. Scaling Sentence Embeddings with Large Language Models\n",
    "The paper \"Scaling Sentence Embeddings with Large Language Models\" explores methods to enhance sentence embeddings using large language models (LLMs) like OPT. The study introduces a prompt-based method with explicit one-word limitations to improve sentence representation. It shows that in-context learning can significantly enhance the quality of sentence embeddings, especially as the model size increases​ (ar5iv)​. https://ar5iv.labs.arxiv.org/html/2307.16645 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering, T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer, AutoModelForDocumentQuestionAnswering\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Causal Model Generator Class\n",
    "class CausalModelGenerator:\n",
    "    def __init__(self, model_name: str, device=None):\n",
    "        self.device = device if device else torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    def generate(self, question: str, context: str = None, max_new_tokens=38, num_beams=3, do_sample=True, top_p=0.92):\n",
    "        try:\n",
    "            if context:\n",
    "                prompt = f\"Given \\' {context} \\' Answer {question}\"\n",
    "            else:\n",
    "                prompt = f\"{question}\"\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    do_sample=True,\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs.get('attention_mask'),\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=num_beams,\n",
    "                    max_length = 200,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=False)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating text for {self.model_name}: {e}\")\n",
    "            return f\"Error generating text for {self.model_name}\"\n",
    "\n",
    "# Seq2Seq Model Generator Class\n",
    "class Seq2SeqModelGenerator:\n",
    "    def __init__(self, model_name: str, device=None):\n",
    "        self.device = device if device else torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def generate(self, context: str, question: str, max_length=200, num_beams=3):\n",
    "        try:\n",
    "            if not question:\n",
    "                raise ValueError(\"Seq2Seq models require both context and question.\")\n",
    "            prompt = f\"Given {context} Answer the question: {question} \"\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=max_length,\n",
    "                    num_beams=num_beams,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return decoded_output\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating text for {self.model_name}: {e}\")\n",
    "            return f\"Error generating text for {self.model_name}\"\n",
    "\n",
    "\n",
    "# QA Model Generator Class\n",
    "class QAModelGenerator:\n",
    "    def __init__(self, model_name: str, device=None):\n",
    "        self.device = device if device else torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def generate(self, context: str, question: str):\n",
    "        try:\n",
    "            if not question:\n",
    "                raise ValueError(\"QA models require both context and question.\")\n",
    "            inputs = self.tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            answer_start = torch.argmax(outputs.start_logits)\n",
    "            answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "            return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating text for {self.model_name}: {e}\")\n",
    "            return f\"Error generating text for {self.model_name}\"\n",
    "\n",
    "# T5 Model Generator Class\n",
    "class T5ModelGenerator:\n",
    "    def __init__(self, model_name: str, device=None):\n",
    "        self.device = device if device else torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "\n",
    "    def generate(self, context: str, question: str, max_length=150, num_beams=4):\n",
    "        try:\n",
    "            input_text = f\"Answer the question: {question} Given context: {context}\"\n",
    "            inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=max_length,\n",
    "                    num_beams=num_beams,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return decoded_output\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating text for {self.model_name}: {e}\")\n",
    "            return f\"Error generating text for {self.model_name}\"\n",
    "        \n",
    "class DocumentQAModelGenerator:\n",
    "    def __init__(self, model_name: str, device=None):\n",
    "        self.device = device if device else torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def generate(self, context: str, question: str):\n",
    "        try:\n",
    "            context_tokens = context.split()\n",
    "            question_tokens = question.split()\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                question_tokens,\n",
    "                context_tokens,\n",
    "                return_tensors=\"pt\",\n",
    "                is_split_into_words=True  \n",
    "            ).to(self.device)\n",
    "            bbox = torch.tensor([[[0, 0, 1000, 1000]] * len(inputs[\"input_ids\"][0])], dtype=torch.int).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=inputs['input_ids'], bbox=bbox, attention_mask=inputs['attention_mask'])\n",
    "            answer_start = torch.argmax(outputs.start_logits)\n",
    "            answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "            answer = self.tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating text for {self.model_name}: {e}\")\n",
    "            return f\"Error generating text for {self.model_name}\"\n",
    "\n",
    "class ModelGeneratorFactory:\n",
    "    distilbert_66M = {\"name\": \"distilbert-base-uncased-distilled-squad\", \"size\": \"66M\"}\n",
    "    blenderbot_90M = {\"name\": \"facebook/blenderbot-90M\", \"size\": \"90M\"}\n",
    "    gpt_2_124M = {\"name\": \"gpt2\", \"size\": \"124M\"}\n",
    "    gpt_neo_125M = {\"name\": \"EleutherAI/gpt-neo-125M\", \"size\": \"125M\"}\n",
    "    roberta_base_125M = {\"name\": \"deepset/roberta-base-squad2\", \"size\": \"125M\"}\n",
    "    bart_base_139M = {\"name\": \"facebook/bart-base\", \"size\": \"139M\"}\n",
    "    layoutLMv2_base_200M = {\"name\": \"microsoft/layoutlmv2-base-uncased\", \"size\": \"200M\"}\n",
    "    t5_base_220M = {\"name\": \"t5-base\", \"size\": \"220M\"}\n",
    "    bert_large_340M = {\"name\": \"bert-large-uncased-whole-word-masking-finetuned-squad\", \"size\": \"340M\"}\n",
    "    gpt_2_medium_355M = {\"name\": \"gpt2-medium\", \"size\": \"355M\"}\n",
    "    pegasus_xsum_568M = {\"name\": \"google/pegasus-xsum\", \"size\": \"568M\"}\n",
    "    gpt_2_large_762M = {\"name\": \"gpt2-large\", \"size\": \"762M\"}\n",
    "    dialoGPT_large_762M = {\"name\": \"microsoft/DialoGPT-large\", \"size\": \"762M\"}\n",
    "    t5_large_770M = {\"name\": \"t5-large\", \"size\": \"770M\"}\n",
    "    gpt_neo_1_3B = {\"name\": \"EleutherAI/gpt-neo-1.3B\", \"size\": \"1.3B\"}\n",
    "\n",
    "    causal_models = [\n",
    "        blenderbot_90M,\n",
    "        gpt_2_124M, \n",
    "        gpt_neo_125M, \n",
    "        gpt_2_medium_355M, \n",
    "        gpt_2_large_762M,\n",
    "        # dialoGPT_large_762M,\n",
    "        gpt_neo_1_3B,  \n",
    "    ]\n",
    "    seq2seq_models = [\n",
    "        bart_base_139M,\n",
    "        pegasus_xsum_568M\n",
    "    ]\n",
    "    qa_models = [\n",
    "        distilbert_66M, \n",
    "        roberta_base_125M, \n",
    "        bert_large_340M\n",
    "    ]\n",
    "    document_qa_models = [\n",
    "        layoutLMv2_base_200M\n",
    "    ]\n",
    "    t5_models = [\n",
    "        t5_base_220M,\n",
    "        t5_large_770M \n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_generator(model: Dict[str, str], device=None):\n",
    "        if model in ModelGeneratorFactory.causal_models:\n",
    "            return CausalModelGenerator(model[\"name\"], device)\n",
    "        elif model in ModelGeneratorFactory.seq2seq_models:\n",
    "            return Seq2SeqModelGenerator(model[\"name\"], device)\n",
    "        elif model in ModelGeneratorFactory.qa_models:\n",
    "            return QAModelGenerator(model[\"name\"], device)\n",
    "        elif model in ModelGeneratorFactory.t5_models:\n",
    "            return T5ModelGenerator(model[\"name\"], device)\n",
    "        elif model in ModelGeneratorFactory.document_qa_models:\n",
    "            return DocumentQAModelGenerator(model[\"name\"], device)\n",
    "        else:\n",
    "            raise ValueError(f\"Model name {model['name']} is not recognized or supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_experiment(llms_list_in, question_in, retrieved_documents_in, device_in, output_file_in = None, model_type_in=\"Causal\"):\n",
    "    results = []\n",
    "\n",
    "    for llm in llms_list_in:\n",
    "        single_result = {}\n",
    "        single_result[\"modelType\"] = model_type_in\n",
    "        single_result[\"llm\"] = llm[\"name\"]\n",
    "        single_result[\"size\"] = llm[\"size\"]\n",
    "        single_result[\"question\"] = question_in\n",
    "        single_result[\"retrieved_documents\"] = retrieved_documents_in   \n",
    "        try:\n",
    "            generator = ModelGeneratorFactory.create_generator(llm, device_in)\n",
    "            single_result[\"LLM Answer\"] = generator.generate(question=question_in, context=None)\n",
    "            single_result[\"RAG Answer\"] = generator.generate(question=question_in, context=retrieved_documents_in)\n",
    "            results.append(single_result)\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error creating generator for {llm}: {e}\"\n",
    "            print(error_message)\n",
    "            single_result[\"LLM Answer\"] = error_message\n",
    "            single_result[\"RAG Answer\"] = error_message\n",
    "            results.append(single_result)\n",
    "        if(output_file_in):\n",
    "            save_dict_to_json_file(results, output_file_in)\n",
    "    return results\n",
    "    # Save the results to the specified output file\n",
    "   \n",
    "\n",
    "def clean_and_join_sentences(sentences):\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not re.search(r'[.!?]$', sentence):\n",
    "            sentence += '.'\n",
    "        cleaned_sentences.append(sentence)\n",
    "    paragraph = \" \".join(cleaned_sentences)\n",
    "    return paragraph\n",
    "import re\n",
    "\n",
    "def clean_and_combine_sentences(sentences):\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and sentence[0].isupper():\n",
    "            sentence = sentence[0].lower() + sentence[1:]\n",
    "        sentence = re.sub(r'[.!?]$', '', sentence)\n",
    "        cleaned_sentences.append(sentence)\n",
    "    combined_sentence = \" and \".join(cleaned_sentences)\n",
    "    combined_sentence += '.'\n",
    "    return combined_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#### Experiment 1\n",
    "# Question Type: Fact-based Question\n",
    "# Question: \"How tall is the Pyramid of Giza?\"\n",
    "# Retrieval type: Sentence retrieval\n",
    "# Retrievals : 1\n",
    "\n",
    "question = \"How tall is the Pyramid of Giza?\"\n",
    "\n",
    "experiment_info = {\n",
    "    \"Question Type\": \"Fact-based Question\",\n",
    "    \"Question\": question,\n",
    "    \"Retrieval type\": \"Sentence retrieval\",\n",
    "    \"Retrievals\": 1\n",
    "}\n",
    "\n",
    "sentence_retriever_in = Retriever()\n",
    "sentence_retriever_in.add_documents(read_and_split_sentences(\"facts_sentences\"))\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "dictionaryModels = {\n",
    "    \"casual_model\": ModelGeneratorFactory.causal_models, \n",
    "    \"qa_model\": ModelGeneratorFactory.qa_models ,\n",
    "    \"t5_model\": ModelGeneratorFactory.t5_models \n",
    "    }\n",
    "results = []\n",
    "results.append(experiment_info)\n",
    "for llms_key in dictionaryModels:\n",
    "    single_llm_result = run_llm_experiment(\n",
    "        model_type_in = llms_key,\n",
    "        llms_list_in = dictionaryModels[llms_key],\n",
    "        question_in = experiment_info[\"Question\"],\n",
    "        retrieved_documents_in =clean_and_join_sentences(sentence_retriever_in.retrieve_documents(experiment_info[\"Question\"], experiment_info[\"Retrievals\"])),\n",
    "        device_in = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "    results.append(single_llm_result)\n",
    "save_dict_to_json_file(results, \"experiments/Experiment1.json\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#### Experiment 2\n",
    "# Question Type: List-based Question\n",
    "# Question : What materials were used in constructing the Great Wall of China?\n",
    "# Retrieval type: Sentence retrieval\n",
    "# Retrievals : 1\n",
    "\n",
    "question = \"What materials were used in constructing the Great Wall of China?\"\n",
    "\n",
    "experiment_info = {\n",
    "    \"Question Type\": \"List-based Question\",\n",
    "    \"Question\": question,\n",
    "    \"Retrieval type\": \"Sentence retrieval\",\n",
    "    \"Retrievals\": 1\n",
    "}\n",
    "\n",
    "sentence_retriever_in = Retriever()\n",
    "sentence_retriever_in.add_documents(read_and_split_sentences(\"facts_sentences\"))\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "dictionaryModels = {\n",
    "    \"casual_model\": ModelGeneratorFactory.causal_models, \n",
    "    \"qa_model\": ModelGeneratorFactory.qa_models ,\n",
    "    \"t5_model\": ModelGeneratorFactory.t5_models \n",
    "    }\n",
    "results = []\n",
    "results.append(experiment_info)\n",
    "for llms_key in dictionaryModels:\n",
    "    results_key = run_llm_experiment(\n",
    "        model_type_in = llms_key,\n",
    "        llms_list_in = dictionaryModels[llms_key],\n",
    "        question_in = experiment_info[\"Question\"],\n",
    "        retrieved_documents_in =clean_and_join_sentences(sentence_retriever_in.retrieve_documents(experiment_info[\"Question\"], 1)),\n",
    "        device_in = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "    results.append(results_key)\n",
    "save_dict_to_json_file(results, \"experiments/Experiment2.json\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#### Experiment 3\n",
    "# Question Type: Fact-based Question\n",
    "# Question: How tall is the Pyramid of Giza?\n",
    "# Retrieval type: Paragraph retrieval\n",
    "# Retrievals: 1\n",
    "\n",
    "question = \"How tall is the Pyramid of Giza?\"\n",
    "experiment_info = {\n",
    "    \"Question Type\": \"Fact-based Question\",\n",
    "    \"Question\": question,\n",
    "    \"Retrieval type\": \"Paragraph retrieval\",\n",
    "    \"Retrievals\": 1\n",
    "}\n",
    "sentence_retriever_in = Retriever()\n",
    "sentence_retriever_in.add_documents(read_and_split_paragraphs(\"facts_sentences\"))\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "dictionaryModels = {\n",
    "    \"casual_model\": ModelGeneratorFactory.causal_models, \n",
    "    \"qa_model\": ModelGeneratorFactory.qa_models ,\n",
    "    \"t5_model\": ModelGeneratorFactory.t5_models \n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(experiment_info)\n",
    "for llms_key in dictionaryModels:\n",
    "    result_llm = run_llm_experiment(\n",
    "        model_type_in = llms_key,\n",
    "        llms_list_in = dictionaryModels[llms_key],\n",
    "        question_in = experiment_info[\"Question\"],\n",
    "        retrieved_documents_in =clean_and_join_sentences(sentence_retriever_in.retrieve_documents(experiment_info[\"Question\"], experiment_info[\"Retrievals\"])),\n",
    "        device_in = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "    results.append(result_llm)\n",
    "save_dict_to_json_file(results, \"experiments/Experiment3.json\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#### Experiment 4\n",
    "# Question Type: List-based Question\n",
    "# Question: What materials were used in constructing the Great Wall of China?\n",
    "# Retrieval type: Paragraph retrieval\n",
    "# Retrievals : 1\n",
    "\n",
    "question = \"What materials were used in constructing the Great Wall of China?\"\n",
    "\n",
    "experiment_info = {\n",
    "    \"Question Type\": \"List-based Question\",\n",
    "    \"Question\": question,\n",
    "    \"Retrieval type\": \"Paragraph retrieval\",\n",
    "    \"Retrievals\": 1\n",
    "}\n",
    "\n",
    "sentence_retriever_in = Retriever()\n",
    "sentence_retriever_in.add_documents(read_and_split_paragraphs(\"facts_sentences\"))\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "dictionaryModels = {\n",
    "    \"casual_model\": ModelGeneratorFactory.causal_models, \n",
    "    \"qa_model\": ModelGeneratorFactory.qa_models ,\n",
    "    \"t5_model\": ModelGeneratorFactory.t5_models \n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(experiment_info)\n",
    "for llms_key in dictionaryModels:\n",
    "    result_llm = run_llm_experiment(\n",
    "        model_type_in = llms_key,\n",
    "        llms_list_in = dictionaryModels[llms_key],\n",
    "        question_in = experiment_info[\"Question\"],\n",
    "        retrieved_documents_in =clean_and_join_sentences(sentence_retriever_in.retrieve_documents(experiment_info[\"Question\"],  experiment_info[\"Retrievals\"])),\n",
    "        device_in = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "    results.append(result_llm)\n",
    "save_dict_to_json_file(results, \"experiments/Experiment4.json\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating text for facebook/blenderbot-90M: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=38) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#### Experiment 5\n",
    "# Question Type: Synthesis-based Question\n",
    "# Question: Which famous structures, both designed or structurally influenced by Gustave Eiffel?\n",
    "# Retrieval type: Paragraph retrieval\n",
    "# Retrievals : 3\n",
    "\n",
    "question = \"Which famous structures, both designed or structurally influenced by Gustave Eiffel?\"\n",
    "experiment_info = {\n",
    "    \"Question Type\": \"Synthesis-based Question\",\n",
    "    \"Question\": question,\n",
    "    \"Retrieval type\": \"Paragraph retrieval\",\n",
    "    \"Retrievals\": 3\n",
    "}\n",
    "\n",
    "sentence_retriever_in = Retriever()\n",
    "sentence_retriever_in.add_documents(read_and_split_paragraphs(\"facts_sentences\"))\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "dictionaryModels = {\n",
    "    \"casual_model\": ModelGeneratorFactory.causal_models, \n",
    "    \"qa_model\": ModelGeneratorFactory.qa_models ,\n",
    "    \"t5_model\": ModelGeneratorFactory.t5_models \n",
    "    }\n",
    "results = []\n",
    "results.append(experiment_info)\n",
    "for llms_key in dictionaryModels:\n",
    "    results_llm = run_llm_experiment(\n",
    "        model_type_in = llms_key,\n",
    "        llms_list_in = dictionaryModels[llms_key],\n",
    "        question_in = experiment_info[\"Question\"],\n",
    "        retrieved_documents_in =clean_and_join_sentences(sentence_retriever_in.retrieve_documents(experiment_info[\"Question\"], experiment_info[\"Retrievals\"])),\n",
    "        device_in = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "    results.append(results_llm)\n",
    "save_dict_to_json_file(results, \"experiments/Experiment5.json\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chatgpt.com/c/9a28b447-7b00-41f3-8b2d-7b42fe293cd3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
